异常处理原则：被大量工程实践所检验过的异常处理黄金原则：任何设计阶段考虑到的异常情况，一定会在系统实际运作中发生，但系统实际运行遇到的异常却可能在设计时未考虑到，所以，除非需求指标允许，在系统设计时不能放过任何异常情况。



# 分布式系统



### 什么是分布式系统

分布式系统是：硬件或软件组件**分布在不同的网络计算机上**，彼此之间通过消息传递来进行通信和协调系统，分布式系统简单来说：就是一群独立的计算机集合共同对外提供服务，但是对于这个系统的用户来说，系统就像是一台计算机提供服务一样

分布式意味着：可以采用更多的普通计算机（相对于昂贵的大型机）组成分布式集群对外提供服务。计算机越多，CPU、内存、存储资源等也就越多，能够处理的并发访问量也就越大。

各个主机之间的通信和协调主要通过网络进行，所以分布式系统中的计算机在空间上几乎没有任何限制，这些计算机可能被放在不同的机房、可能放在不同的城市中，甚至大型的网站甚至分布在不同国家和地区



### 分布式系统主要特征

分布式系统无论空间上怎么分布，一个标准的分布式系统应该具有以下几个重要特征

#### 分布性

分布式系统中的多台计算机之间在空间位置上可以随意分布，同时，机器的分布情况也会随时变动

#### 对等性

分布式系统中的计算机**没有主/从之分**。

副本(Replica)是分布式系统中场景的概念，指的是：分布式系统对数据和服务提供的一种冗余方式，常见的分布式系统中，为了对外提供高可用的服务，往往会对数据和服务进行副本处理

数据副本：不同节点上持久化同一份数据，当某一个节点上存储的数据丢失时，可以从副本上读取该数据，这是解决分布式系统数据丢失问题最为有效的手段

服务副本：多个节点提供同样的服务，每个节点都有能力接收来自外部的请求并进行相应的处理

#### 自治性

分布式系统中，各个节点都包含自己的处理机和内存，各自具有独立处理数据和服务的能力。通常，彼此的地位是平等的，没有主次之分，既能自治地进行工作，又能利用共享的通信线路来传递信息，协调任务处理

#### 并发性

同一个分布式系统中的多个节点，可能会并发地操作一些共享的资源，如何准确并高效地协调分布式并发操作也成为了分布式系统架构与设计中最大的挑战之一





### 分布式理论基础：CAP、BASE

#### CAP

CAP理论是分布式系统，特别是分布式存储领域中被讨论最多的理论

C、A、P三者不能同时满足，最多满足其中的两项，P是前提(分布式系统中，网络分区问题始终存在)，然后在C和A之间选择。

##### 一致性Consistency

1. 分布式环境中，一致性指数据在多个分布式节点中要能保持一致的特性，即：更新操作成功后，所有节点在同一时间的数据完全一致。
2. 假如客户端对其中一个节点的数据进行了更新操作，却没有使得其他节点的数据更新，这样就会出现各个节点之间的数据不一致，会导致客户端读到脏数据，如果能使其他节点的数据得到相应的更新，保证客户端**每次读到都是最新的数据，此系统就认为具有强一致性**
3. 一致性是在并发读写的时候才会出现的问题，因此在**理解一致性问题时，一定要注意结合考虑并发读写的场景**

##### 可用性Availability

可用性指：系统节点只要不宕机的情况下，那么该系统24小时对每一个请求操作都提供服务，在有限的时间对请求作出响应，称此系统具有高可用性



##### 分区容错性Partition tolerance

能容忍网络分区，在网络断开的情况下，被分隔的节点仍能正常对外提供服务

网络分区

- 分布式系统中，多个节点之前的网络本来是连通的，但是因为某些故障（例如部分节点网络出现故障），导致某些节点之间不同了，整个网络就被分成了几个区块，这就是网络分区
- 分布式集群中，节点之间由于网络不通，导致集群中的节点形成不同的子集，子集中节点间的网络相通。但是，子集与子集间网络不同。也可以说，网络分区是子集与子集之间在网络上互相隔离了

判断网络出现分区：判断节点之间心跳是否超时，将心跳可达的节点归属到一个子集中



1. 分区容错性是指分布式系统在遇到任何网络分区故障的时候，仍然需要保证系统对外提供满足一致性和可用性服务，只要不是整个网络环境发生了故障
2. 分区容错性和扩展性紧密相关，在分布式应用中，分区容错性高指：在部分节点故障或出现丢包的情况下，集群系统仍然能提供服务，完成数据的访问。分区容错可视为在系统中采用多副本策略

例如：三个网络环境 北京、上海、深圳。部署在北京的机房网络出现故障，不影响上海和深圳的机房，上海和深圳的服务器还是能对外提供服务

![](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/img/image-20220311132145766.png)



##### C、A、P之间的相互关系

> 当发生网络分区的时候，如果我们要继续提供服务，那么强一致性和可用性就只能满足一个。
>
> 也就是说：当网络分区之后P是前提，决定了P之后才有C和A的选择
>
> 也就是说：分区容错性是必须要实现的



CA：

- **分布式系统中分区问题是始终存在的**，因此CA的分布式系统更多的是允许分区后各个子系统依然保持CA

- 例如：如果出现了分区后，系统的某个节点在进行写操作，此时为了满足C，则必须禁止其他节点读写请求操作，这就和A冲突了。如果为了保证A，其他节点的读写请求正常的话，这就和C发生冲突了

  

CP：如果**不要求可用性**，相当于每个请求都需要在各个服务之间强一致性，也就是同步过程中，这个系统不对外提供服务，等系统同步完成后，再对外提供服务



AP：放弃强一致性，一旦分区发生，节点之间可能会失去联系，导致各个节点数据不一致。为了实现高可用，每个节点只能用本地数据提供服务，而这样就会导致全局数据不一致，请求的用户读到旧数据，对系统的影响不大



**为什么选择两个特性的原因**：为什么CAP理论，只能保其中两个。上面是一张简单的分布式系统图，两两互相通信。
如果CAP三个都满足。假设A，B，C三个服务各自保存了用户信息数据，现在用户对A发起了更新用户信息操作，那么服务A更新成功后，需要同步一下服务B和服务C，将服务B和服务C中的用户信息也进行更新，那么在服务A同步的过程中，可能就会出现用户对服务B发起读操作，那么用户可能就会读到还没有更新的数据(脏数据)，也就会出现数据不一致，不满足**C(一致性)**。如果要满足C的话，那就是在同步的过程中，整个分布式系统对外不提供服务，等数据同步完成后，再对外提供服务，这样就能满足C(一致性)了，但是这样又不满足**A(可用性)**，因为系统在这段时间内不对外提供服务。

如果要同时满足CA的话，我们可以将三个服务做成一个系统，也就是不想要进行同步操作，这样的话数据在一个系统中，这样即能满足一致性，又能对外提供服务，但是这样的话因为做成了一个系统，也就不存在分布式的概念，也就是不满足**P(分区容错性)**，已经没有了分区的概念了。所以一个分布式系统至多满足其中的两项



CP、AP之间如何选择

> 关键还是要看业务需求

比如需要保证强一致性的场景，比如银行一般会采用CP





##### CAP结论

一个分布式系统基本上要保证P，那么只能在CA做取舍了，具体是舍弃C还是A的话，可以由具体的业务而定，如果我们的业务需要保证强一致性的话，那么我们就可以舍弃A可用性，来保证C了。而如果我们的系统对数据的一致性要求不高，那么我可以舍弃C，来保证系统的可用性A了。**像zookeeper就是保证了CP，舍弃了A。像eurekA就是保证AP，舍弃了C。**针对CAP，后面又出现BASE理论。

---



#### BASE

1. Base理论是在CAP理论上进行改进的，对CAP中的一致性和可用性权衡的结果，既然CAP中无法做到强一致性和高可用性，那么可以采用适当的方式使系统达到最终一致性和基本可用的特点
2. Base理论就是：Basically Available(基本可用)、Soft State(软状态)、Eventually consistency(最终一致)，它**通过牺牲强一致性来获取可用性，允许数据在一段时间内是不一致的，但最终达到一致的状态**



Base是对AP的补充，关注最终一致性



##### 基本可用

假如舍弃A，系统无法对外提供服务，我们大可不必将整个系统的功能置为不可用，而是**允许牺牲部分服务的可用性**，其他服务的功能模块还是能对外提供服务，来保证系统的基本可用。

系统的基本可用体现在两个方面：

1. 一个是请求响应时间上的损失，也就是用户获取用户信息的请求可能处理时间比较长，需要等同步操作完成。这样系统还是能对外提供服务。
2. 还有一种就是功能上的损失，也就是在同步的过程中，将获取用户信息的接口禁用掉，不对外提供服务，将用户的请求引导到一个降级页面中，但是系统还是对外提供服务的，保证了系统的基本可用

##### 软状态

指：**允许系统中的数据存在中间状态**，并且认为该中间状态的存在不会影响系统的整体可用性。即允许系统在**不同节点的数据副本之间**进行数据同步的过程存在延时

例如：A节点与A1-replica、A2-replica两个副本之间的数据同步，允许存在延时

##### 最终一致性

最终一致性跟强一致性不同的是，**最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态**。本质是需要系统最终数据能够达到一致，而不需要实时保证系统数据的强一致性

1. 读时修复：查询数据的时候，如果检测到不同节点的副本数据不一致，系统就自动修复数据
2. 写时修复：在写入数据，检测数据的不一致时，进行修复。（推荐 **写时修复**，这种方式对性能消耗比较低）
3. 异步修复：这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复



##### Base理论思想主要的实现

1. 按功能划分数据库，不同功能使用不同的数据库
2. sharding分片



### 分布式一致性算法

> 一致性算法：目的是保证分布式系统中，多数据**副本节点的数据一致性**

1. 强一致性算法

   - Paxos算法
   - Raft
   - ZBA算法
   - 一致性hash算法

2. 弱一致性

   - 系统不保证提交后的数据，在副本间立刻改变状态，而是随着时间的推移，最终状态是一致的

3. 模型

   - DNS系统
   - Gossip系统

   

这些算法暂时不深入了解，晓得有这个东西先，等到2023年再来看吧



### 分布式架构的水平和垂直扩展

#### 垂直伸缩

> 通过升级或增加单台机器的硬件来支持访问量

1. 增加cpu的核心数

   - 增加cpu后系统的服务能力得到提升，响应速度、同时处理的线程数量增多
   - 但是：会造成一些显著问题
     - 锁竞争加剧，更多的线程来访问共享资源
     - 支持并发的请求的线程是固定的，此时即使增加了cpu，系统也不会得到提升

2. 增加内存

   - 增加内存能直接提升系统的响应速度，但是有可能达不到效果，就是在JVM堆大小没有调整

   

好处：技术难度低，运营和改动成本相对较低

缺点：机器的性能是有瓶颈的，同时升级小型或大型机器成本比较大

#### 水平伸缩

> 通过添加机器来支持访问量

水平伸缩理论上来讲，没有什么瓶颈

缺点是：成本高、给运维带来了更大的挑战









# RPC

### RPC是什么

1. 远程过程调用协议（Remote Procedure Call），是一种通过网络从远程计算机上请求服务，而不需要了解底层网络技术的协议，是分布式系统常见的一种通信方法。
   - 例如：两台服务器A、B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数/方法，由于不在一个内存空间，所有不能直接调用，需要通过网络来表达调用的语义和传达调用的数据
2. 过程是什么：过程就是业务处理、计算任务，更直白说就是程序，就是像调用本地方法一样调用远程的过程
3. ![img](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/img/45366c44f775abfd0ac3b43bccc1abc3_1440w.jpg)



### Client Stub

> 客户端存根或客户端桩，存放着服务端的地址消息

将客户端的请求参数打包成网络消息，然后通过网络远程发给服务端



### Server Stub

> 服务端存根或服务端桩
>
> 接收客户端发送过来的消息，将消息解包，并调用本地方法



### RPC和本地调用区别

远程调用需要通过网络，所有响应比本地调用要慢几个数量级，也不那么可靠



### RPC模式

RPC采用客户端/服务端的模式，通过request-response消息模式实现



### RPC三个过程

1. 通讯协议
   - 首先，要解决通讯的问题，主要是通过在客户端和服务器之间建立TCP连接，远程过程调用所有交换的数据都在这个连接里传输。
   - 连接可以是按需连接，调用结束后就断掉，也可以是长连接，多个远程过程调用共享同一个连接
2. 寻址问题
   - 也就是说，A服务器上的应用怎么告诉底层的RPC框架，如何连接到B服务器（如主机或IP地址）以及特定的端口、方法的名称名称是什么，这样才能完成调用。
   - 比如基于Web服务协议栈的RPC，就要提供一个endpoint(端点) URI，或者是从UDDI服务上查找。如果是RMI调用的话，还需要一个RMI Registry来注册服务的地址
3. 数据序列化
   - 当A服务器上的应用发起远程过程调用时，方法的参数需要通过底层的网络协议如TCP传递到B服务器，由于网络协议是基于二进制的，内存中的参数的值要序列化成二进制的形式，也就是序列化（Serialize）或编组（marshal），通过寻址和传输将序列化的二进制发送给B服务器
   - B服务器收到请求后，需要对参数进行反序列化，恢复为内存中的表达方式，然后找到对应的方法（寻址的一部分）进行本地调用，然后得到返回值
   - 返回值还要发送回服务器A上的应用，也要经过序列化的方式发送，服务器A接到后，再反序列化，恢复为内存中的表达方式，交给A服务器上的应用

### 为什么要是使用RPC

1. 服务化/微服务
2. 分布式系统架构
3. 服务可重用
4. 系统间交互调用







### RPC的调用流程

1. 客户端处理过程中调用client stub，就像调用本地方法一样，传入参数
2. client stub将参数编组(序列化)为消息，然后通过系统调用向服务端发送消息
3. 客户端本地的操作系统将消息从客户端发送到服务端
4. 服务端接收的数据包传递给server stub
5. server stub将接收到的数据解组为参数
6. server stub再调用服务端的本地方法，将本地方法执行的结果以反方向的相同步骤响应给客户端

stub(存根、桩)：分布式计算中存根是一段代码，它转换在远程过程调用期间client和server之间传递的参数



### RPC协议

RPC调用过程中需要将消息进行编组然后发送，接收方需要解组消息为参数，过程处理结果也需要经过编组、解组；消息由哪些部分构成以及消息的表示 形式就构成了消息协议。

RPC协议规定请求消息、响应消息的格式，在TCP之上我们可以选用或自定义消息协议来实现RPC的交互



### RPC框架

封装好了参数编组、消息解组、底层网络通信的RPC程序开发框架，可以直接在此基础上编写。常见的RPC框架：Dubbo、Motan(新浪微博)、SpringCloud(openfeign伪RPC)、apache Thrift(facebook)、gRPC(google)等等



### RPC特性

1. RPC一般使用长连接，不必每次通信都要进行3次握手，减少网络开销
2. RPC一般都有注册中心、有丰富的监控管理
3. RPC协议更简单、效率更高，服务化治理、服务化架构，RPC框架是一个强力的支撑



### 服务暴露

远程服务提供者需要以某种形式提供服务调用的相关信息，服务调用者通过一定的途径获取远程访问调用的相关信息

### 远程代理对象

服务调用者使用的服务实际上是远程服务的本地代理，说白了就是通过动态代理实现

### 通信

RPC框架的通信与具体的协议无关，具体可基于Http或Tcp协议实现



### 序列化

传输方式和序列化会之间影响RPC的性能，不同的Rpc框架可以在序列化方式上采用不同的技术，从而提高性能

#### 常用序列化方式

JDK原生序列化：Serializable接口

Json序列化：Jackson、Gson、FastJson

Hessian序列化

- 是一个支持跨语言传输的二进制文本序列化协议，对比`Java`默认的序列化，`Hessian`的使用较简单，并且性能较高，现在的主流远程通讯框架几乎都支持`Hessian`，比如`Dubbo`，默认使用的就是Hessian，不过是Hessian的重构版

ProtoBuf序列化

- 是一种轻便高效的结构化数据存储格式，可以用于结构化数据序列化。适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式

Kyro序列化：广泛使用在大数据组件



### RPC与Http

#### 相同点

1. 都是基于TCP协议的点对点通信
2. 都可以在不同编程语言(应用系统)间进行通信

#### 不同点

1. 数据编码格式不同
2. 一般情况下：RPC是长连接，Http是短链接
3. 一般情况下：Rpc的传输效率高于Http



### 如何实现一个RPC框架

思路：

1. 服务设计：客户端、服务端、注册中心
   - 如何知道服务端的消息
   - 如何调用
2. 服务端
   - 注册到注册中心
3. 客户端
   - 从注册中心拉取服务端的接口信息
4. Rpc调用过程
   - 客户端->通过动态代理调用服务端接口
   - -> 选取不同的调用策略-> 异步方式调用
   - -> 服务端-> 客户端监听接收结果-> 关闭连接



# SpringCloud



### 什么是微服务架构

> 将传统的单体的应用，分为多个应用程序，每一个应用程序就是一个微服务模块，服务之间相互调用
>
> 每个微服务运行在自己的进程里，使用轻量级的机制通信

例如：dubbo(只用来做微服务)、springcloud(还提供了服务发现、断路器等等组件)



### 微服务的特点

1. 按照业务划分为一个独立运行的应用程序
2. 服务之间通过Http协议相互通信
3. 可以使用不同的编程语言、存储技术等实现自己的模块
4. 服务集中化管理、配置
5. 微服务是一个分布式系统



### 微服务面临的问题

1. 服务划分：如何将一个完整的服务拆分为多个服务，是一件比较困难的事情
2. 服务性能：微服务间通过Rest、Rpc等形式进行交互，通信时延受道较大的影响
3. 分布式事务：不同的服务可能采用了不同的数据库，无法利用数据库本身提供的事务来保证一致性，需要引入二阶段提交等等技术
4. 服务部署：服务的部署方式，最佳方式为docker、K8s
5. 运维复杂：需要设计一个良好的监控系统对各个微服务的运行状态进行监控。通常是：普罗米修斯+Grafa



### SpringCloud是什么

> springcloud是一系列框架的有序集合
>
> 它利用springboot的开发便利性巧妙地简化了分布式系统基础设施的开发



### SpringBoot和SpringCloud区别

1. springboot专注于快速方便的开发单个个体微服务
2. springcloud关注全局的微服务协调整理治理框架，它将springboot开发的一个个单体微服务整合并管理
3. 为各个微服务之间提供：配置管理、服务发现、断路器、路由、分布式回话等等集成服务
4. springboot可以离开springcloud独立使用开发项目，但是springcloud离不开springboot，属于依赖的关系









---

### Eureka



#### 服务注册和发现是什么

> 服务注册：将提供某个服务的模块信息，注册到一个公共的组件上去。例如：Eureka、Nocas、zookeeper、consul
>
> 服务发现：注册的这些服务模块，能及时的被其他调用者发现，不管是新增、删除都能实现自动发现



![image-20220713142819372](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202207131429772.png)

#### 什么是Eureka

1. 作为springcloud的服务注册功能服务器，它是服务注册中心，系统中的其他服务使用Eureka的客户端将其注册到Eureka Service中，并且保持心跳
1. 提供了完整的服务注册服务发现
2. 可以通过Eureka Service来监控各个微服务是否运行正常

#### Eureka两大组件

Eureka采用C/S架构。

1. Eureka Server:服务注册中心，主要用于提供服务注册功能。当微服务启动时，会将自己的服务注册到Eureka Server。Server维护一个可以服务列表，存储了所有注册到Eureka Server的可以服务的信息，这些服务可以在Server管理界面直观看到
2. Eureka Client:客户端，通常指的是微服务系统中各个微服务，主要用于和Server进行交互。微服务应用启动后，Client会向Server发送心跳（默认周期30s）。若Server在多个心态周期内没有收到某个Client的心跳，Server会将它从可用服务列表中移除（默认90s）



#### Eureka的服务注册与发现

1. 服务注册register

   - Eureka客户端通过Rest请求向Eureka服务端注册自己的服务，提供自身的元数据，例如：服务的ip地址、端口、运行状况等等

   - Eureka服务端接收到注册信息后，会把这些元数据信息存储在一个两层的Map中，第一层是appName，第二层是InstanceInfoId

   - ~~~java
     ConcurrentHashMap<String,Map<String,Lease<InstanceInfo>>> registry
     ~~~

Eureka Server通过服务名和InstanceInfoId来区分一个服务实例



所有服务都在Eureka服务器上注册并通过调用Eureka服务器完成服务查找发现

主启动类上添加@EnableEurekaServer开启服务



#### Eureka心跳及自我保护机制

> Eureka心跳：即服务续约(renew)，服务注册后，Eureka客户端和Eureka服务端会维护一个心跳来持续通知Eureka服务器，说明服务一直处于可以状态，防止被剔除



1. 应用服务启动后，各节点会像Eureka Server发送心跳，默认周期为30s，如果Eureka Server在多个心跳周期(默认90s）没有收到某个节点的心跳，Eureka Server将会从服务注册列表中把这个服务节点移除。

2. Eureka服务器向客户端公开下面资源以让其发送心跳

   - PUT /eureka/apps/{app id}/{instance id}?status={status}
   - {instance id}采用 hostname:app id:port，其中app id代表标识唯一的Eureka客户端实例，Eureka服务器会识别一些状态数值：UP; DOWN; STARTING; OUT_OF_SERVICE; UNKNOWN.

3. 客户端发送心跳时的URL 例如：

   PUT /eureka/apps/ORDER-SERVICE/localhost:order-service:8886?status=UP
   
4. 自我保护机制

   - Eureka Server会定时剔除超时没有续约的服务，那就有可能出现一种场景，网络一段时间内发生了异常，所有的服务都没能够进行续约，Eureka Server就把所有的服务都剔除了，这样显然不太合理
   - 所以有了自我保护机制，当短时间内，统计续约失败的比例，达到一定的阈值就触发自我保护机制，该机制下不会剔除任何服务，等到正常后退出自我保护机制
   - Eureka Server在运行期间会去统计心跳成功的比例在15分钟之内是否低于85% , 如果低于85%， Eureka Server会认为当前实例的客户端与自己的心跳连接出现了网络故障，那么Eureka Server会把这些实例(节点)保护起来，让这些实例不会过期导致实例剔除。
   - 这样做就是为了防止Eureka Client可以正常运行, 但是与Eureka Server网络不通情况下， Eureka Server不会立刻将Eureka Client服务剔除
   - 自我保护模式下，Eureka Service会保护服务注册表中的信息，不再删除注册表中的数据，当网络故障恢复后，Eureka Service节点会自动退出自我保护模式

5. 关闭自我保护机制，保证不可用服务及时剔除

   ~~~yaml
   eureka:
     server:
     	enable-selt-preservation: false
   ~~~



#### 获取服务(get registry)

1. 服务消费者在启动的时候，会发送Rest请求给Eureka服务端，获取上面注册的服务清单，并且缓存在Eureka客户端本地，默认缓存30s
2. 为了性能Eureka服务端会维护一份只读的服务清单缓存，该缓存每隔30s刷新一次

##### DiscoverClient作用

可以从注册中心**根据服务名 获取注册在Eureka上的服务的信息**



#### 服务调用

1. 服务消费者在获取到服务清单后，根据清单中的服务列表信息查找到其他服务的地址，从而进行远程调用
2. Eureka有个Region和Zone的概念，一个Region包含多个Zone，在进行服务调用时，优先访问处于同一个Zone中的访问提供者
   - 提供了Region和Zone两个概念来进行分区，region可以理解为地理上的不同，比如亚洲、中国、深圳等等
   - zone：可以理解为Region里面具体的机房，比如region为深圳，然后深圳有两个机房zone1、zone2



#### 服务下线(cancel)

1. 当Eureka客户端需要关闭或重启时，这段时间内不希望有请求进来，所以需要提前发送Rest请求和Eureka服务端，告诉Eureka服务端自己要下线了
2. Eureka收到请求后，会把该服务状态设置为下线(Down)，并把该下线事件传播出去

#### 服务剔除(evict)

1. 服务实例可能因为网络故障等原因导致不能提供服务，而且此时该实例也没有发送请求给Eureka服务端来进行服务下线，所以此时需要服务的剔除机制
2. Eureka服务端在启动时，会创建一个定时任务，每隔一段时间(默认60s)，从当前服务清单中把超时(90s)没有续约的服务剔除
2. **该机制主要是应对非正常关闭服务的情况**。例如：内存溢出、服务器宕机等非正常流程关闭服务时



#### Eurek遵循AP原则

Eureka Server各个节点都是平等的，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查找发现服务，客户端向某个Eureka服务端注册或查询服务时，如果发现连接失败，则会自动切换到其他可用服务节点，只要有一台服务可用，就能保证服务的可用性。只不过查到的可能不是最新的(不保证强一致性)，即消费者可能获取到过期的服务列表

Eureka Client 会拉取、更新和缓存 Eureka Server 中的信息。因此当所有的 Eureka Server 节点都宕掉，服务消费者依然可以使用缓存中的信息找到服务提供者，但是当服务有更改的时候会出现信息不一致



#### Eureka如何实现高可用

Eureka Server集群，相互注册，不同Eureka Server之间进行访问同步，用来保证访问信息的一致性



#### Eureka server集群间数据同步方式

分布式系统数据在多个副本之间的复制方式主要有：

1. 主从复制

   有一个主副本，其他为从副本，所有写操作都提交到主副本，再由主副本更新到其他从副本。写压力主要集中在主副本上，是系统的瓶颈，从副本可用分担读请求

2. 对等复制

   副本之间不分主从，任何副本都可以接收写操作，然后每个副本间互相进行数据更新。对等复制模式下，任何副本都可以接收写请求，不存在写压力瓶颈，但是各个副本间的数据同步时可能产生数据冲突

其中，**Eureka Server集群采用对等复制模式**，节点通过彼此相互注册来提高可用性，Server每当自己的信息变更后，就会把自己的最新信息通知给其他Eureka Server保持数据同步

Server集群之间采用异步方式同步，所以节点之间数据不是强一致性的，不过，基本上能保证最终一致性





#### Eureka工作流程

1. Eureka Server 启动成功，等待服务端注册。在启动过程中如果配置了集群，集群之间定时通过 Replicate 同步注册表，每个 Eureka Server 都存在独立完整的服务注册表信息
2. Eureka Client 启动时根据配置的 Eureka Server 地址去注册中心注册服务
3. Eureka Client 会每 30s 向 Eureka Server 发送一次心跳请求，证明客户端服务正常
4. 当 Eureka Server 90s 内没有收到 Eureka Client 的心跳，注册中心则认为该节点失效，会注销该实例
5. 单位时间内 Eureka Server **统计到有大量的 Eureka Client 没有上送心跳**，则认为可能为网络异常，进入自我保护机制，不再剔除没有上送心跳的客户端
6. 当 Eureka Client 心跳请求恢复正常之后，Eureka Server 自动退出自我保护模式
7. Eureka Client 定时全量或者增量从注册中心获取服务注册表，并且将获取到的信息缓存到本地
8. 服务调用时，Eureka Client 会先从本地缓存找寻调取的服务。如果获取不到，先从注册中心刷新注册表，再同步到本地缓存
9. Eureka Client 获取到目标服务器信息，发起服务调用
10. Eureka Client 程序关闭时向 Eureka Server 发送取消请求，Eureka Server 将实例从注册表中删除 



### Consul

作为注册中心使用，属于CP



### Zookeeper

1. zk作为注册中心需要jdk的支持
2. zk注册服务时，存储的是临时节点



### Eureka和Zookeeper区别

1. Eureka满足AP

   - Eureka Server集群之间，各个节点是平等的，其中的某个/些节点挂了不会影响正常的工作，剩余的节点依然可以提供服务注册和查询
   - Eureka客户端向某个Eureka服务端注册的时候，如果发现注册失败了，则会自动切换到其他节点
   - 只要还有一台Eureka Server是正常工作的，就能保证服务是可用的。不过不能保证查到的信息是最新的
   - 通过这种方式满足AP，但是数据不是强一致性的

2. Zookeeper满足CP

   但是ZooKeeper会出现这样一种情况，当Master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30 ~ 120s，且选举期间整个ZooKeeper集群都是不可用的，这就导致在选举期间注册服务瘫痪。在云部署的环境下，因网络问题使得ZooKeeper集群失去Master节点是较大概率会发生的事，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的







---

### Ribbon

> ribbon是Netflix发布的开源项目
>
> 主要功能是**提供客户端软件的负载均衡算法和服务调用(并不是它发起调用，而是它决定调用哪个服务)**
>
> Ribbon客户端组件提供了一系列完善的配置项，根据配置信息，Ribbon会自动基于某种规则(如：轮询、随机等）去连接不同的机器。

 

#### 负载均衡

> 简言之：将高并发、高流量的网络请求，经过负载均衡技术，动态转发给后端的多个节点进行处理，以提高响应效率、提高吞吐量



##### 软件负载均衡

nginx：Nginx是**服务器负载均衡**，客户端所有请求都会交给nginx,然后由nginx实现转发请求。即负载均衡是由服务端实现的

LVS、HAProxy等



##### 硬件负载均衡

> 直接在服务器和外部网络之间安装负载均衡设备，这种硬件设备通常称为负载均衡器
>
> 由于使用专门的设备完成网络请求转发的任务，独立于操作系统，整体性能高，负载均衡策略多样化、流量管理智能化

F5、Array、NetScaler等

优点：其功能强大、直接连接交换机,处理网络请求能力强，与系统无关，负载性很强。可以应用于大量设施、适应大访问量、使用简单

缺点：价格高昂、成本高





##### 负载均衡类型

集中式负载均衡：在消费者和服务提供方中间使用独立的代理方式进行负载，有硬件的（比如 F5），也有软件的（比如 Nginx）

进程内负载均衡：将LB逻辑集成到消费方,消费方从服务注册中心获知有哪些地址可用，然后自己再从这些地址中选择出一个合适的服务器。Ribbon就属于进程内LB,它只是一个类库, 集成于消费方进程,消费方通过它来获取到服务提供方的地址。

~~~yaml
#ribbon超时配置
ribbon:
  OkToRetryOnAllOperations: false #对所有操作请求都进行重试,默认false,包括连接超时(connectTimeout)和请求超时(readTimeOut)
  ReadTimeout: 3000   #负载均衡超时时间，默认值5000
  ConnectTimeout: 2000 #ribbon请求连接的超时时间，默认值2000
  MaxAutoRetries: 0     #对当前实例的重试次数，默认0,不包含首次调用
  MaxAutoRetriesNextServer: 0 #对切换实例的重试次数，默认1,不包含首次调用
  
  
  #熔断配置
  hystrix:
  command:
    default:
      execution:
        timeout:
          enabled: true
        isolation:
          thread:
            timeoutInMilliseconds: 60000 #默认1000ms



#feign超时配置
feign:
  hystrix:
    enabled: true #默认为false，如果为false,则使用ribbon的超时设置和重试机制，否则使用feign的相关设置
  client:
    config:
      default:
        #连接到目标的时间，此处会收到注册中心启动中的影响。设置为3秒钟，如果注册中心有明显的不在线，基本是毫秒级熔断拒绝
        connectTimeout: 3000
        #获取目标连接后执行的最长时间，设置为32秒，即服务最长时
        readTimeout: 32000

#结论
根据上面分析，Hystrix的熔断时间要大于Feign或Ribbon的connectTimeout+readTimeout，
由于ribbon的重试次数为RetryCount = (maxAutoRetries + 1) * (maxAutoRetriesNextServer + 1)，因此必须保证(maxAutoRetries + 1) * (maxAutoRetriesNextServer + 1)*（ConnectTimeout+ReadTimeout）< timeoutInMilliseconds，因为如果小于超时时间, 那就熔断了, 没有机会重试了
~~~

为什么要设置请求超时、连接超时等等？

1. 如果不设置超时时间，用户操作相关接口时，将会出现长时间的无响应，严重影响用户体验
2. 负载很高的系统：大量调用耗时长的接口，导致性能急剧下降，从而影响其他正常的业务



#### Ribbon实现原理

使用discoverClient**从注册中心读取目标服务器信息**，对同一接口请求进行计数，使用取余算法获取目标服务集群索引，返回获取到的目标服务信息



#### Ribbon实现负载均衡

Ribbon 是一个客户端的负载均衡器，它可以与 Eureka 配合使用轻松地实现客户端的负载均衡。Ribbon 会先从 Eureka Server（服务注册中心）去获取服务端列表，然后通过负载均衡策略将请求分摊给多个服务端，从而达到负载均衡的目的

Ribbon本地负载均衡，在调用微服务接口时候，会在注册中心上获取注册信息服务列表之后缓存到JVM本地,从而在本地实现RPC远程服务调用技术。



Ribbon核心组件：IRule 里面的choose方法

SpringCloud Ribbon 提供了一个IRule接口，该接口主要用来定义负载均衡策略，他有7个默认实现类，每一个实现类都是一个负载均衡策略

| 序号 | 实现类                    | 负载均衡策略                                                 |
| ---- | ------------------------- | ------------------------------------------------------------ |
| 1    | RoundRobinRule            | 按照线性轮询策略，即按照一定的顺序依次选取服务实例           |
| 2    | RandomRule                | 随机选取一个服务实例                                         |
| 3    | RetryRule                 | 按照 RoundRobinRule（轮询）的策略来获取服务，如果获取的服务实例为 null 或已经失效，则在指定的时间之内不断地进行重试（重试时获取服务的策略还是 RoundRobinRule 中定义的策略），如果超过指定时间依然没获取到服务实例则返回 null 。 |
| 4    | WeightedResponseTimeRule  | WeightedResponseTimeRule 是 RoundRobinRule 的一个子类，它对 RoundRobinRule 的功能进行了扩展。  根据平均响应时间，来计算所有服务实例的权重，响应时间越短的服务实例权重越高，被选中的概率越大。刚启动时，如果统计信息不足，则使用线性轮询策略，等信息足够时，再切换到 WeightedResponseTimeRule。 |
| 5    | BestAvailableRule         | 继承自 ClientConfigEnabledRoundRobinRule。先过滤点故障或失效的服务实例，然后再选择并发量最小的服务实例。 |
| 6    | AvailabilityFilteringRule | 先过滤掉故障或失效的服务实例，然后再选择并发量较小的服务实例。 |
| 7    | ZoneAvoidanceRule         | 默认的负载均衡策略，综合判断服务所在区域（zone）的性能和服务（server）的可用性，来选择服务实例。在没有区域的环境下，该策略与轮询（RandomRule）策略类似。 |

Spring Cloud Ribbon**默认使用轮询策略选取服务实例**，我们也可以根据自身的需求切换负载均衡策略



#### RestTemplate

1. RestTemplate作用是发送http请求的客户端，用于2个服务之间的请求发送
2. 其简化了Rest Api调用，只需要使用它的一个方法，就可以完成请求、响应、Json转换

API

1. getForObject/getForEntity  (url，转换的类型.class，提交的参数)

   ![image-20220406212604863](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204062126157.png)

​		![image-20220406212830912](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204062128387.png)

1. postForObject/postForEntity   （url，协议体数据，转换的类型.calss)

RestTemplate并没有限制Http的客户端类型，目前常用的三种都支持：HttpClient、OkHttp、Jdk原生的UrlConnection



RestTemplate不+Ribbon也可以调用，只是没有负载均衡而已

RestTemplate+Ribbon可实现 ：带负载均衡的远程调用

~~~java
@Configuration
public class ApplicationContext {
    @Bean
    @LoadBalanced
    public RestTemplate GetrestTemplate(){
        return new RestTemplate();
    }
}
~~~





项目怎么使用的？

自定义RibbonConfiguration，然后自定义负载均衡规则-》

在选择server后，整合自定义的Nocas负载规则进行选择

---



### Feign

> 声明式服务调用组件，我们只需要声明一个接口并通过注解进行简单的配置（类似与Dao接口上面的Mapper注解一样）即可实现对http接口的绑定
>
> 不需要我们自己去构建Http请求了，直接调用接口就行了



它底层是基于http协议，是一个http请求调用的轻量级框架，可以以Java接口注解的方式调用Http请求，Feign通过处理注解，将请求模板化，当时就调用的时候，传入参数，再应用到请求上，从而转化成真正的请求，封装了http调用流程



Feign默认使用JDK原生的URLConnection发送Http请求，没有连接池、对每个请求地址会保持一个长连接。若采用原生HttpUrlConnection没有连接池、性能和效率比较低，可能会遇到性能问题导致系统崩溃

建议换用Apache HttpClient作为底层的http client包，从而获取连接池、超时时间等与性能相关的控制能力，或者采用okHttpClient



Feign真正发送Http请求是委托为feign.client来做的

~~~yaml
#换底层http客户端
feign.httpclient.enabled: true

#也可以使用okhttp
feign:
	okhttp:
		enable: true
~~~



使用方法：定义一个服务接口，然后在上面添加注解

~~~java
//name和url同时指定的时候，url生效
//url：指定了默认值为 空（注意冒号），当配置信息feign.url.qianchao不存在时，就会走注册中心根据服务名来查找
@Feignclient(name="qianchaoProduct",url="${feign.url.qianchao:}")
public interface QianchaoTestApi{
    
}
~~~



原理：

1. 程序启动时，会扫描所有包下@FeignClient注解的类，并将这些类注入到Spring Ioc容器中，当定义的Fiegn中的接口被调用时，通过Jdk动态代理为其生成RequestTemplate
2. RequestTemplate中包含了请求的所有信息。例如：请求参数、请求url等等
3. 



Feign内置了一个重试器，当Http请求出现IO异常时，会有一个最大尝试次数发送请求





#### SpringCloud服务间通信方式

Feign(集成了ribbon)

RestTemplate+ribbon

1. ribbon需要我们自己构建Http请求，模拟http请求，然后通过RestTemplate发送给其他服务，步骤繁琐
2. Fiegn则是在Ribbon的基础上进行了一次封装改进，采用接口的方式，将我们需要调用的服务方法定义成抽象方法保存在本地就可以了，不需要自己构建Http请求了，直接调用接口就行了，不过要注意，调用方法要和本地抽象方法的签名完全一致



这两种方式都是采用Restful API接口调用服务的http接口，参数和结果都采用默认的jackson进行序列化和反序列化

- RestTemplate是Spring提供的用于访问Rest服务的客户端
- 它在Http客户端库(例如：HttpURLConnection、HttpComponents、okHttp等)的基础上，封装了简单易用的模板方法API
- restTemplate处理异常：ResponseErrorHandler、DefaultResponseErrorHandler等几个类



#### 重试组件

Retryer是负责重试的组件，Feign内置了重试器，当Http请求出现异常时，Feign会限定一个最大重试次数来进行重试操作。默认重试机制是关闭的

什么情况下启用重试机制：一定要考虑到服务端的接口是否支持幂等性，才可以考虑重试问题。如果不确定是否可以安全的重试的时候，那么就不要重试，快速失败总比问题扩大更好



#### Feign超时配置控制

其服务调用以及负载均衡是依靠底层Ribbon实现的，因此超时控制通过Ribbon来设置

~~~yaml
ribbon:
  ReadTimeout: 6000  #建立连接后，服务器读取到可用资源的时间 
  ConnectionTimeout: 6000  #建立连接所用的时间，适用于网络状况正常的情况下，两端连接所用的时间
~~~



#### InvocationHandlerFactory

InvocationHandlerFactory 采用 JDK 的动态代理方式生成代理对象，我们定义的 Feign Client 是接口，当我们调用这个接口中定义的方法时，实际上是要去调用远程的 HTTP API，这里用了动态代理方式，当调用某个方法时，会进入代理中真正的去调用远程 HTTP API



#### RequestInterceptor请求拦截器

可以外Feign添加多个拦截器，在请求执行前设置一些扩展的参数信息



#### RestTemplate和Feing调用服务的区别

1. 调用方式不同
   - RestTemplate需要我们自己构建http请求，模拟http请求然后发给其他服务，步骤繁琐
   - Feign在Ribbon基础上进行了一次封装，采用接口的方式，将需要调用的服务方法定义成抽象方法保存在本地就可以了，不需要自己构建http请求，直接调用接口就可以了。不过需要调用方法要和本地抽象方法的签名完全一致



---

### OpenFeign

它是 Spring 官方推出的一种声明式服务调用与负载均衡组件，它的出现就是为了替代进入停更维护状态的 Feign

核心作用：为Http形式的RestAPI提供了简洁高效的伪RPC调用方式，像调用一个本地方法一样，调用远端的RestApi接口，完全是RPC形式

1. 里面集成了ribbon、feign的封装、hystrix(默认是关闭的)
1. feign不支持SpringMvc注解，所有OpenFeign在Feign的基础上支持SpringMvc的注解，例如@RequestMapping等等
1. @FeignClient可以解析SpringMVC的@RequestMapping注解下的接口，并通过动态代理的方式产生实现类，实现类中做负载均衡并调用其他服务
2. 其服务调用以及负载均衡是依靠底层Ribbon实现的，因此超时控制通过Ribbon来设置
3. OpenFeign里面默认没有开启Hystrix
3. 底层是对http调用组件的封装，底层为okhttp、httpclient等等

#### 常用注解

@EnableFeignClient

启动类上添加，表示开启OpenFeign功能，SpringCloud启动时，会去扫描标记@FeignClient注解的接口生成代理并且注入到Spring容器中

@FeignClient

该注解用于通知OpenFeign组件对@RequestMapping注解下的接口进行解析，并通过动态代理的方式产生实现类，实现负载均衡和服务调用

#### 工作原理

1. spring扫描@FeignClient，生成对应接口的代理对象（实现类，其还会做负载均衡），并注入Spring容器中。当远程接口的方法被调用时，由代理实例去完成真正的远程访问
   - openFeign使用JDK动态代理，在构建BeanDefinition的时候，传入FeignClientFactoryBean类型，FeiginClient会被动态代理成为一个FeignClientFactoryBean，放在容器中
2. 生成的实现（代理）类如何适配各种Http组件？
   - FeignAutoConfiguration的条件注解
3. 生成的实现/代理类如何实现Http请求应答序列化和反序列化
   - Feign提供了Decoder和Encoder两个接口
   - 使用Jackson对http进行序列化与反序列化（feign底层还是http、okhttp等）
4. 生成的实现/代理类如何注入Spring容器
   - @EnableFeignClients
   - 扫描@FeignClient，给这些接口创建代理对象，并将代理对象注入Spring容器，再使用时候其实是对代理对象的使用

当请求发生时，对接口的调用，被统一转发到Feign实现的InvacationHandler中，由其负责将接口中的入参等信息转换为http形式，发送到服务器，最后再解析Http响应，将结果转换为对象后返回







---



### 幂等性问题

Http/1.1中幂等性的定义：一次和多次请求某一个资源对于资源本身应该具有同样的结果(网络超时等问题除外)。也就说，任意多次执行对资源本身所产生的影响均与一次执行的影响相同

#### 什么情况下需要幂等（场景）

业务中经常会遇到重复提交的情况，无论是网络问题无法收到请求结果而重新发起请求，还是前端操作造成重复提交情况。

交易、支付系统中：这种重复提交尤为明显：用户连续点击多次提交订单，后台应该只产生一个订单

声明幂等的服务认为，外部调用者会存在多次调用的情况，为了防止外部多次调用对系统数据的状态发生多次改变，将服务设计成幂等

1. 前端重复提交
2. 接口超时重试
3. 消息重复消费
   - 使用消息中间件来处理消息队列，且手动ack确认消息被正常消费时。如果消费者突然断开连接，那么已经执行一半的消息会重新放回队列。
   - 当消息被其他消费者重新消费时，如果没有幂等性就会导致消息重复消费时结果异常

#### 接口为什么要实现幂等

重复提交，后台只产生对应这个数据的一个反应结果



#### 幂等性核心思想

通过唯一的业务单号保证幂等



#### 防重复提交策略（解决方案）

1. 乐观锁

2. 防重表（防止数据重复的表）

   - 实现方式利用mysql唯一索引的特性
   - ![image-20220414205029514](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204142050629.png)
   - 流程：
   - 建立一张去重表，标准某个字段建立唯一索引，客户端请求服务器，服务端把这次请求的一些信息插入到这张表中
   - 因为表中某个字段是唯一索引，所以如果插入成功则表明没有这次请求的信息，继续执行请求的业务逻辑。若插入失败则表示已经执行过当前请求，直接返回

   

3. 分布式锁

   - 基于redis的setNX命令
   - setnx key v ：将key的值为v，当且仅当key不存在。若给定的key存在，则setnx不做任何操作。该命令设置成功时返回1，失败时返回0
   - ![image-20220414205530289](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204142055465.png)
   - 流程：
   - 客户端先请求服务端，会拿到一个能代表这次请求业务的唯一字段
   - 将该字段以 SETNX 的方式存入 redis 中，并根据业务设置相应的超时时间
   - 如果设置成功，证明这是第一次请求，则执行后续的业务逻辑
   - 如果设置失败，则代表已经执行过当前请求，直接返回

4. token令牌
   - 当客户端请求页面时，服务器会生成全局唯一的ID作为token，并保存在redis
   - 然后再次请求业务接口时，把token携带过去，一般放在请求头部。
   - 服务器会校验 token，校验成功则执行业务，并删除redis中的token
   - 如果判断token不存在redis中，就表示是重复操作，直接返回重复标记给client，这样就保证了业务代码，不被重复执行
   - ![image-20220414204506569](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204142045280.png)





#### Get、Post、Put、Delete的幂等性

1. get用于获取资源，不会有副作用，所以是幂等的
2. post常用于往数据库添加、修改数据，每调用一次会产生新的数据，数据经常发生变化，所以不是幂等的
3. put常用于创建和更新指定的一条数据，如果数据不存在则新建，如果存在则更新数据，多次和一次调用产生的副作用是一样的，所以是满足幂等
4. delete调用一次或多次对系统产生的副作用是一样的，所以是幂等的

#### 应该在那一层进行幂等性设计

第一层：app、pc等等

第二层：负载均衡

第三层：网关层，主要做路由转发、请求鉴权、身份认证、限流等。如果在网关层实现幂等性，那需要把业务代码写在网关层一般不推荐

第四层：业务层：主要处理业务逻辑，所以也不合适

第五层：持久层：和数据库打交道，这块不做的话可能对数据产生一定的影响，所以一般是在持久层做幂等性校验

第六层：数据库层





#### 项目中如何实现防止重复提交的

https://www.cnblogs.com/gavincoder/p/14357892.html

1. 自定义注解（可以设置失效时间）
2. 定义切面、拦截Controller进行环绕通知
3. 设置key，存放与redis
4. 判断redis中是否有key来进行防止重复提交

---



### Hystrix

> 断路器：Spring Cloud Hystrix是一款优秀的服务容错与保护组件，它提供了熔断器功能，能有效地阻止分布式微服务系统中出现联动故障，以提高微服务系统的弹性。
>
> Hystrix具有服务降级、服务熔断、线程隔离、请求缓存、请求合并、以及实时故障监控等强大功能

能够保证一个依赖出错的情况下，不会导致整体服务的失败，避免级联故障，提高分布式系统的弹性



#### 微服务中，如何保护服务

1. 一般采用Hystrix框架，实现服务隔离避免出现服务的雪崩效应，从而达到保护服务的效果
2. 当出现服务不可用，使用服务降级返回友好提示，从而避免单个服务崩溃引发整体服务的不可用



#### 服务雪崩效应

1. 当一个服务调用另一个服务时，由于网络原因或自身原因等等出现问题，调用者就会等待被调用者的响应，当更多的服务请求到这些资源导致更多的服务等待，发生连锁反应(雪崩效应)
2. 当某个服务发生宕机时，调用这个服务的其他服务也会发生宕机，这样就会将服务的不可用逐步扩大到各个其他服务中，从而使整个项目的服务宕机崩溃，发生服务雪崩
3. 发生雪崩的原因：单个服务存在bug、请求访问量激增导致服务崩溃、服务器的硬件故障等等



#### Hystrix容错设计的手段

1. 服务降级：接口调用失败就调用本地的一个方法进行相关处理
2. 服务熔断：接口调用失败就进入提前定义好的一个熔断方法，返回错误信息
3. 服务隔离：隔离服务之间的相互影响
4. 服务监控：发生服务调用时，会将每秒请求数、成功请求数等指标记录下来，自带一个Dashboard页面可以看到详细信息，但是一般不用它的



#### 服务降级(fallback)

> 服务**降级是从整个系统的负荷情况考虑的**，对某些负荷比较高的情况，为了预防某些功能出现负荷过载或响应慢的情况，在其内部暂时会舍弃对一些非核心的接口和数据的请求，而直接返回一个提前准备好的fallback信息。这样虽然提供的是一个有损的服务，但是却保证了整个服务的稳定性和可用性
>
> 降级的目的：是应对系统自身的故障
>
> 核心思想：整体系统负载高的情况下，丢车保帅，优先保证核心业务

1. 当某些服务不可用时，为了避免长时间等待造成服务卡顿或者雪崩效应，而主动执行备用的服务降级逻辑，立即返回一个友好的提示，以保障主体业务不受影响，fallback
2. Hystrix实现服务降级的功能是通过重写HystrixCommand中的getFallback方法，或者HystrixObservableCommand的resumeWithFallback()方法，使服务支持服务降级

什么情况下会触发服务降级

1. 程序运行异常
2. 超时
3. 服务熔断发生服务降级
4. 线程池、信号量也会导致服务降级



熔断和降级的区别？

1. 相同
   - 目标一致：都是从可用性和可靠性出发，为了防止系统崩溃
   - 用户体验类似：用户体验到的都是某些功能暂时不可用
2. 不同
   - 触发原因不同：熔断一般是某个服务(下游服务)故障引起的；降级一般是从整体负荷考虑



#### 服务熔断

> 类似与家用保险丝，**当某服务不可用或响应超时的情况时**，为了防止整个系统出现服务雪崩，暂时停止对该服务的调用
>
> 熔断的目的：应对依赖的外部服务出现故障的情况

##### 服务雪崩效应

服务调用中通常会设计多层服务的调用，服务间通过网络进行通信，从而支持整个系统的运行。

然而任务服务都不是100%可用的，网络通常也是比较脆弱的

当服务提供者的不可用导致服务调用者不可用，并将这种行为逐渐放大的现象称为服务雪崩



在服务降级的基础上更直接的一种保护方式，当在一个统计时间范围内的请求失败数量达到设定值或当前的请求错误率达到设定的错误率阈值时开启断路，之后的请求直接走fallback()方法，在设定时间后尝试恢复。

类似与保险丝达到最大访问后，直接拒绝访问，拉闸限电，然后调用访问降级的方法来返回友好提示

![image-20220406231253626](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204062312840.png)

1. 熔断机制是**为了应对雪崩效应**而出现的一种微服务链路保护机制
2. 当微服务系统中的某个微服务不可用或响应时间太长时，为了保护系统的整体可用性，熔断器会暂时切断请求对该服务的调用，并快速返回一个友好的错误响应。这种熔断状态不是永久的，在经历了一定的时间后，熔断器会再次检测该微服务是否恢复正常，若服务恢复正常则恢复其调用链路

具体配置：[参考](https://blog.csdn.net/weixin_44449838/article/details/110765752)



##### 断路器三个重要参数

1. 快照时间戳
   - 断路器是否打开需要统计一些请求和错误数据，统计的时间范围就是快照时间窗，默认是最近的10s
2. 请求总数阈值
   - 在快照时间窗内，必须满足请求总数阈值才有资格熔断。默认是20
   - 意味着：10秒内，如果Hystrix命令的调用次数不足20次，即使所有的请求都失败，断路器也不会打开
3. 错误百分百阈值
   - 当请求总数在快照时间窗内超过了阀值，比如发生30次调用，有15次发生了超时，也就是超过50%的错误比例，这时候会将断路器打开



##### 熔断状态及原理

1. 断路器三种状态
   - 打开状态:断路器完全打开，那么下次请求就不会请求该服务，而是快速返回定义的失败响应
   - 半开状态：一段时间后（默认5s)，这个时候断路器是半开状态，会将部分请求转发给该服务，正常调用时，断路器关闭
   - 关闭状态：当服务一直处于正常状态，服务能正常调用
   
2. ![熔断状态转换](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/img/10162355X-7.png)

3. SpringCloud中，熔断机制通过Hystrix实现。Hystrix会监控微服务间的调用情况，当失败调用到一定比例时，就会启动熔断机制

4. Hystrix 实现服务熔断的步骤如下：
   1. 当服务的调用请求失败数量达到设定值(默认10s内超过20次请求)达到或超过 Hystix 规定的比率（默认为10s内超过 50%请求失败）后，熔断器进入熔断开启状态。
   2. 熔断器进入熔断开启状态后，Hystrix 会启动一个休眠时间窗，在这个时间窗内，该服务的降级逻辑会临时充当业务主逻辑，而原来的业务主逻辑不可用。
   3. 当有请求再次调用该服务时，会直接调用降级逻辑快速地返回失败响应，以避免系统雪崩。
   4. 当休眠时间窗到期后（默认5s），Hystrix 会进入半熔断转态，允许部分请求对服务原来的主业务逻辑进行调用，并监控其调用成功率。
   5. 如果调用成功率达到预期，则说明服务已恢复正常，Hystrix 进入熔断关闭状态，服务原来的主业务逻辑恢复；否则 Hystrix 重新进入熔断开启状态，休眠时间窗口重新计时，继续重复第 2 到第 5 步。
   
   

#### 服务隔离

服务隔离：使用线程池、信号量隔离的目的是：为不同的服务分配一定的资源，当自己的资源用完，直接返回失败而不是占用别人的资源

Hystrix为隔离的服务开启一个独立的线程池，这样在高并发的情况下不会影响其他服务。服务隔离有线程池和信号量两种实现方式，一般使用线程池方式实现

![image-20230405170547453](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202304051705425.png)



线程池隔离：

图的左边2/3是线程池资源隔离示意图，右边的1/3是信号量资源隔离示意图

我们先来看左边的示意图。 当用户请求服务A和服务I的时候，tomcat的线程(图中蓝色箭头标注)会将请求的任务交给服务A和服务I的内部线程池里面的线程(图中橘色箭头标注)来执行，tomcat的线程就可以去干别的事情去了，当服务A和服务I自己线程池里面的线程执行完任务之后，就会将调用的结果返回给tomcat的线程，从而实现资源的隔离，当有大量并发的时候，服务内部的线程池的数量就决定了整个服务的并发度，例如服务A的线程池大小为10个，当同时有12请求时，只会允许10个任务在执行，其他的任务被放在线程池队列中，或者是直接走降级服务，此时，如果服务A挂了，就不会造成大量的tomcat线程被服务A拖死，服务I依然能够提供服务。整个系统不会受太大的影响



信号量隔离：

信号量的资源隔离只是起到一个开关的作用，例如，服务X的信号量大小为10，那么同时只允许10个tomcat的线程

此处是tomcat的线程，而不是服务X的独立线程池里面的线程来访问服务X，其他的请求就会被拒绝，从而达到限流保护的作用



---

### Spring Cloud Config

> 作为分布式系统中的外部配置，提供了服务器和客户端的支持。可以方便的对微服务各个环境下的配置进行集中管理
>
> Config Server负责：读取配置文件（从git等地方），并且暴露Http Api接口
>
> Config Client负责：通过调用Server端暴露的接口，来读取配置文件

[参考地址](http://c.biancheng.net/springcloud/config.html)

分布式微服务体系中，几乎所有服务的运行都离不开配置文件的支持，这些配置文件通常由各个服务自行管理。例如：application.yml等等

这种配置文件散落在各个服务中的管理方式存在以下问题：

1. 管理难度大：配置文件散落在各个微服务中，难以管理
2. 安全性低：配置随着源代码保存在代码库中，容易造成配置泄露
3. 时效性差：微服务中的配置修改后，必须重启服务，否则无法生效
4. 局限性明显：无法支持动态调整、例如:日志开关、功能开关

为了解决以上这些问题，通常使用配置中心对配置进行统一管理



#### 分布式配置中心有哪些框架

百度：Disconf、淘宝：diamond、携程Apollo、zookeeper、spring cloud config



#### 什么是springcloud config

1. Spring Cloud Config为微服务架构中各个微服务各个环境下的配置提供集中化的外部配置支持
2. 简单说就是：SpringCloud Config可以将各个微服务的配置文件集中存储在一个外部的存储仓库或系统(Git、Svn等)中，对配置的统一管理，以支持各个微服务的运行
3. Spring Cloud Config 分为Config Server和Client两部分
4. Server也被称为分布式配置中心，它是一个独立运行的微服务应用，用来连接配置仓库并为客户端提供获取配置信息、加密信息、解密等信息的访问接口
5. Client指的是微服务架构中的各个微服务，他们通过Config Server对配置进行管理，并从Config Server中获取和加载配置信息
6. Spring Cloud Config默认使用Git存储配置信息



#### 分布式配置中心的作用

动态变更项目配置信息，其不必重新部署项目





#### Spring Cloud Config工作原理

![img](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/img/1019425240-0.png)

工作流程：

1. 开发或运维人员提交配置文件到远程的Git仓库
2. Config服务端(Server)负责连接配置仓库git，并对config客户端暴露获取配置的接口
3. config客户端通过config服务端暴露出来的接口，拉取配置仓库中的配置
4. config客户端拉取获取到的配置信息，以支持服务的运行



服务端：分布式配置中心，是一个独立的微服务

客户端：读取配置

将配置信息以Rest接口的形式暴露



#### Spring Cloud Config可以实现实时刷吗

自身不能

SpringCloud Config实时刷新采用SpringCloud Bus消息总线实现



SpringCloud Config和Nacos的区别？

1. Spring Cloud Config 动态刷新需要依赖 Spring Cloud Bus，而 Nacos 则是在后台修改后直接推送到各服务
2. Spring Cloud Config的刷新机制针对所有修改的变量，只要有改动，后台就会获取。而Nacos则是支持粒度更细的方式，只有 refresh 属性为 true 的配置项，才会在运行的过程中变更为新的值。这时Nacos特有的方式

两者共同点

动态刷新的范围都是以下两种：

1. @ConfigurationProperties注解配置的类
2. @RefreshScope注解的bean

---



### Spring Cloud Bus

又称消息总线，它能够通过轻量级的消息代理(例如：RabbitMq、kafka等)将微服务架构中的各个服务连接起来，实现广播状态更改、事件推送等功能，还可以实现微服务间的通信功能

目前SpringCloud Bus支持的消息代理：RabbitMq、Kafka

#### 什么是SpringCloud Bus

1. bus就像是一个分布式执行器，用于扩展SpringBoot应用程序的配置文件，也可以用作应用程序直接的通信通道
2. 不能单独通信，需要配合MQ的支持
3. 一般与SpringCloud Config配合做配置中心

#### SpringCloud Bus基本原理

SpringCloud Bus会使用一个轻量级的消息代理来构建一个公共的消息主题(Topic)，这个Topic中的消息会被所有服务实例监听和消费。当其中的一个服务刷新数据时，SpringCloud Bus会把信息保存到Topic中，这样监听这个Topic的服务就收到消息并自动消费

#### SpringCloud Bus动态属性配置的原理

1. 当Git仓库中的配置发生了改变，我们只需要向某一个服务(既可以是Config服务端，也可以是Config客户端)发送一个post请求，SringCloud Bus就可以通过消息代理通知其他服务重新拉取最新配置，以实现配置的动态刷新
2. ![bus+config 动态刷新配置](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/img/101942GY-11.png)
3. 动态刷新步骤：
   - 当 Git 仓库中的配置发生改变后，运维人员向 Config 服务端发送一个 POST 请求，请求路径为“/actuator/refresh”。
   - Config 服务端接收到请求后，会将该请求转发给服务总线 Spring Cloud Bus。
   - Spring Cloud Bus 接到消息后，会通知给所有 Config 客户端。
   - Config 客户端接收到通知，请求 Config 服务端拉取最新配置。
   - 所有 Config 客户端都获取到最新的配置。



动态刷新配置可以全局通知，也可以定点通知





---

### Spring Cloud Gateway

> 网关相当于一个服务架构的入口，所以的网络请求必须通过网关转发到具体的某一个服务



#### 网关主要作用

统一管理微服务请求，权限控制、负载均衡、路由转发、安全控制、黑白名单控制等等



#### API网关

1. 网关是一个服务器，也可以说是进入系统的唯一节点。网关负责请求转发、合成和协议转换等等
2. 所有来自客户端的请求都要经过网关，然后路由这些请求到对应的微服务
3. 可以在API网关中处理一些非业务功能的逻辑，例如：权限验证、监控、缓存、请求路由等等
4. 常见的API网关实现方案：SpringCloud gateway、zuul、Nginx+Lua、Kong、Traefik



#### 什么是Springcloud gateway

>[参考文章](http://c.biancheng.net/springcloud/gateway.html)
>
>Spring Cloud Gateway是构建在Spirng5生态上，基于WebFlux实现，而WebFlux框架底层使用了Reactor模式通信框架Netty，gateway还支持Websocket并且与Spring紧密集成



1. 作为SpringCloud官方推出的第二代网关框架，取代Zuul网关
2. 网关常见的功能：**路由转发、权限校验、限流控制等等**
3. 基于Spring5、SpringBoot2、Project Reactor等技术开发的高性能API网关组件



![img](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202211212050416.png)



#### gateway基本原理

> 使用了Spring webflux非阻塞网络框架，网络层默认使用了高性能非阻塞的Netty Server，解决了Zuul因为阻塞的线程模型带来的性能下降问题

Gateway本身也是一个SpringBoot应用，它处理请求的逻辑是根据配置的路由，对请求进行与处理了和转发



#### SpringCloud gateway三大核心概念

##### 路由(Route)

1. 网关最基本的模块，由一个Id、一个目标Url、一组断言(Predicate)和一组过滤器(Filter)组成
2. gateway上可以配置多个Routes，处理请求时，按照优先级排序，找到第一个满足所有Predicates的route



##### 断言(Predicate)

断言：表示路由的匹配条件，先于filter执行，因为断言为真时，才进行路由匹配

Predicate就是事先定义好的一组匹配规则，方便请求过来找到对应的Route进行处理

1. 路由转发的判断条件，SpringCloud Gateway通过断言来实现路由的匹配规则，也就是只有满足了断言的条件，才会被转发到指定的服务上进行处理
2. 我们可以通过Predicate的Http请求进行匹配，例如：请求方式、请求路径、请求头、参数等等，如果请求与断言匹配成功，则将请求转发到相应的服务
3. 路由与断言的对应关系为：一对多，当一个请求想要转发到指定路由上时，必须同时满足路由上的所有断言，且请求只会被首个成功匹配的路由转发

##### 过滤器(Filter)

我们可以使用过滤器对请求进行拦截和修改，还可以使用它对上下文的相应进行再处理

###### 过滤器分类：

1. Pre类型：这种过滤器在**请求被转发到微服务之前**可以对请求进行拦截和修改，例如参数校验、权限校验、流量监控、日志输出以及协议转换等操作
2. Post类型：这种过滤器在**微服务对请求做出响应后可以对响应进行拦截和再处理**，例如修改响应内容或响应头、日志输出、流量监控等

作用范围划分：

1. GatewayFilter：应用在单个路由或一组路由上的过滤器

2. GlobalFilter：应用在所有的路由上的过滤器

3. 可以自定义全局过滤器：实现GlobalFileter、复写方法

   <img src="https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204072311361.png" alt="image-20220407231132016" style="zoom:150%;" />





#### SpringCloud Gateway工作流程

gateway启动时，基于Netty Server监听一个指定的端口(该端口可通过server.port自定义)，当客户端发送一个请求到网关时，网关会根据一系列Predicate的匹配结果来决定访问哪个Route，然后根据过滤器链进行请求的处理

![Spring Cloud Gateway 工作流程](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/img/101P45T2-1.png)

1. SpringCloud Gateway启动时会创建Netty Server，由它负责接收来自客户端的请求
2. 收到请求后，根据路由的断言匹配条件找到第一个满足条件的路由
   - gateway通过Gateway Handler Mapping找到与请求相匹配的路由，将其发送给Gateway Web Hander
3. Web Hander通过指定的过滤器链，将请求通过Netty Client转发到实际的服务节点中执行相关业务逻辑，然后返回响应结果
4. 过滤器在转发之前(Pre)或之后(post)可能会执行相关的逻辑
5. 过滤器（Filter）可以在**请求被转发到服务端前**，对请求进行拦截和修改，例如参数校验、权限校验、流量监控、日志输出以及协议转换等
6. 过滤器可以在**响应返回客户端之前**，对响应进行拦截和再处理，例如修改响应内容或响应头、日志输出、流量监控等
7. 响应返回给客户端



#### Spring Cloud Gateway动态路由

默认情况下，gateway会根据服务注册中心中维护的服务列表，以服务名作为路径创建动态路由进行转发，从而实现动态路由功能

将配置文件中Route的uri地址修改为：

~~~yaml
lb://service-name
#lb:uri的协议，表示开启springcloud gateway的负载均衡功能
#service-name:服务名，springcloud gateway会根据它获取到的具体微服务地址
~~~



#### 黑白名单添加

自定义过滤器实现









### 服务链路追踪 Zipkin

Zipkin是一个分布式链路追踪系统，可以采集时序数据来协助定位延迟等相关问题

Sleuth提供了一套完整的服务追踪的解决方案，在分布式系统中提供追踪解决方案并且兼容支持了Zipkin

下载：https://dl.bintray.com/openzipkin/maven/io/zipkin/java/zipkin-server/

运行

服务启动后web端地址：http://localhost:9411/zipkin/

架构图

![image-20220407221143409](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204072212823.png)

#### 结构

zipkin包括四个组件：collector、storage、search、webUI，其中collector中有两个参数

1. span：表示一个追踪节点，有唯一标识
2. Trace：表示一条调用链路，根据Span的parentId串联起来

#### 跟踪web请求

springcloud中，采用sleuth来做跟踪处理，具体通过一个拦截器实现

#### 发送跟踪数据

#### 数据存储

zipkin支持MySQL、ES等存储方式





### 服务链路追踪-SkyWalking

> SkyWalking是一个国产开源的观测平台，用于从服务和云原生等基础设施中收集、分析、聚合以及可视化数据
>
> SkyWalking更像是一种现代化的应用程序性能监控工具，专为云原生、基于容器以及分布式系统而设计



#### 为什么需要链路追踪

> 由于微服务项目拆分，会导致各个服务间调用链路复杂。此时，一个前端请求可能最终需要调用多个后端服务才能完成实现
>
> 1. 链路梳理难
> 2. 故障定位难

有了链路追踪后，微服务间相互依赖关系即使再混乱，每一个请求的调用链路都会被清晰的记录，我们可以快速定位调用链路上某一环节的问题

链路追踪有很多组件，比如：Sleuth、Zipkin、阿里鹰眼、大众点评cat、skywalking等等。这些组件有一个共同的名字：APM工具，即：Application Performance Management，应用性能监控工具





---

### Spring Cloud Stream

#### 为什么引入Cloud Stream

为了不关注具体MQ的细节，只需要用一种适配绑定的方式，自动给我们在各种mq内切换，即：屏蔽底层消息中间件的差异，降低切换成本，同一消息的编程模型，靠的就是Binder

应用程序通过inputs或者outputs来与SpringCloud Stream中的binder对象交互

通过配置来binding，stream的binder对象负责与消息中间件交互

所以只需要搞清楚如何与stream交互就可以方便使用消息驱动的方式



通过使用Spring Integration来连接消息代理中间件以实现消息事件驱动

Stream为了一些供应商的消息中间件产品提供了个性化的自动化配置实现，引用了发布-订阅、消费组、分区三个核心概念

目前支持rabbitmq 、kafka

Springcloud alibaba 加入了Rocket MQ Binder用于RocketMq 集成到SpringCould Stream

系统用到了rabbitmq 、kafka

通过stream来沟通两者

#### cloud stream设计思想

binder input：对应于生产者

binder output：对应于消费者

Stream 应用到了发布-订阅模式

---



# SpringCloud Alibaba

Spring Cloud Alibaba 是阿里巴巴结合自身丰富的微服务实践而推出的微服务开发的一站式解决方案，是 Spring Cloud 第二代实现的主要组成部分

Spring Cloud Alibaba 吸收了 Spring Cloud Netflix 的核心架构思想，并进行了高性能改进。自 Spring Cloud Netflix 进入停更维护后，Spring Cloud Alibaba 逐渐代替它成为主流的微服务框架



### SpringCloud Alibaba三大组件

Nacos：服务注册中心和配置中心

Sentinel：熔断限流

Seata：分布式事务





### Spring Cloud 两代实现组件对比

| Spring Cloud 第一代实现（Netflix） | 状态                                             | Spring Cloud 第二代实现（Alibaba） | 状态                                                 |
| ---------------------------------- | ------------------------------------------------ | ---------------------------------- | ---------------------------------------------------- |
| Ereka                              | 2.0 孵化失败                                     | Nacos Discovery                    | 性能更好，感知力更强                                 |
| Ribbon                             | 停更进维                                         | Spring Cloud Loadbalancer          | Spring Cloud 原生组件，用于代替 Ribbon               |
| Hystrix                            | 停更进维                                         | Sentinel                           | 可视化配置，上手简单                                 |
| Zuul                               | 停更进维                                         | Spring Cloud Gateway               | 性能为 Zuul 的 1.6 倍                                |
| Spring Cloud Config                | 搭建过程复杂，约定过多，无可视化界面，上手难点大 | Nacos Config                       | 搭建过程简单，有可视化界面，配置管理更简单，容易上手 |



### Spring Cloud Alibaba 服务注册与配置中心(Nacos)

[官网地址](https://nacos.io/zh-cn/)

等价于Eureka+config+bus

属于AP模型，也可以配置CP模型

Nacos：Dynamic Naming and Configuration Service，由阿里巴巴团队使用Java语言开发的开源项目。

Nacos是一个更易于帮助云原生应用的动态服务发现、配置和服务管理平台

| 组成部分 | 全称              | 描述                                                         |
| -------- | ----------------- | ------------------------------------------------------------ |
| Na       | naming/nameServer | 即服务注册中心，与 Spring Cloud Eureka 的功能类似。          |
| co       | configuration     | 即配置中心，与 Spring Cloud Config+Spring Cloud Bus 的功能类似。 |
| s        | service           | 即服务，表示 Nacos 实现的服务注册中心和配置中心都是以服务为核心的。 |



Nacos可集成Ribbon，可以使用Ribbon来进行负载均衡

#### Nacos特性

##### 服务发现

Nacos支持基于DNS和RPC的服务发现，当服务提供者向Nacos注册服务后，服务消费者可以在Nacos上通过DNS TODO或Http&API 查找、发现服务

##### 服务健康监测

Nacos提供对服务的实时健康检查，能够阻止请求发送到不健康主机或服务实例上。Nacos提供了一个健康检查仪表盘，帮助我们根据健康状态管理服务的可用性以及流量

##### 动态配置服务

动态配置服务可以让我们以中心化、外部化、动态化的方式，管理所有环境的应用配置和服务配置

Nacos默认动态刷新配置文件，历史版本默认保留30天，还有一键回滚功能

![image-20220408104916236](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/image-20220408104916236.png)

##### 动态DNS服务

Nacos提供动态DNS服务，能够让我们更容易地实现负载均衡、流量控制以及数据中心内网的简单DNS解析服务

---



#### Nacos两大组件

与Eureka类似，Nacos也采用C/S架构

| 组件                                                         | 描述                                                         | 功能                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Nacos Server                                                 | Nacos 服务端，与 Eureka Server 不同，Nacos Server 由阿里巴巴团队使用 Java 语言编写并将 Nacos Server 的下载地址给用户，用户只需要直接下载并运行即可。 | Nacos Server 可以作为服务注册中心，帮助 Nacos Client 实现服务的注册与发现。 |
| Nacos Server 可以作为配置中心，帮助 Nacos Client 在不重启的情况下，实现配置的动态刷新。 |                                                              |                                                              |
| Nacos Client                                                 | Nacos 客户端，通常指的是微服务架构中的各个服务，由用户自己搭建，可以使用多种语言编写。 | Nacos Client 通过添加依赖 spring-cloud-starter-alibaba-nacos-discovery，在服务注册中心（Nacos Server）中实现服务的注册与发现。 |
| Nacos Client 通过添加依赖 spring-cloud-starter-alibaba-nacos-config，在配置中心（Nacos Server）中实现配置的动态刷新。 |                                                              |                                                              |

#### Nacos服务注册中心

Nacos 作为服务注册中心可以实现服务的注册与发现，流程如下图。



![Nacos 服务注册与发现](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/img/1022563360-0.png)
图1：Nacos 服务注册与发现


在图 1 中共涉及到以下 3 个角色：

- 服务注册中心（Register Service）：它是一个 Nacos Server，可以为服务提供者和服务消费者提供服务注册和发现功能。
- 服务提供者（Provider Service）：它是一个 Nacos Client，用于对外服务。它将自己提供的服务注册到服务注册中心，以供服务消费者发现和调用。
- 服务消费者（Consumer Service）：它是一个 Nacos Client，用于消费服务。它可以从服务注册中心获取服务列表，调用所需的服务。


Nacos 实现服务注册与发现的流程如下：

1. 从 Nacos 官方提供的下载页面中，下载 Nacos Server 并运行。
2. 服务提供者 Nacos Client 启动时，会把服务以服务名（spring.application.name）的方式注册到服务注册中心（Nacos Server）；
3. 服务消费者 Nacos Client 启动时，也会将自己的服务注册到服务注册中心；
4. 服务消费者在注册服务的同时，它还会从服务注册中心获取一份服务注册列表信息，该列表中包含了所有注册到服务注册中心上的服务的信息（包括服务提供者和自身的信息）；
5. 在获取了服务提供者的信息后，服务消费者通过 HTTP 或消息中间件远程调用服务提供者提供的服务

#### Nacos配置中心

Nacos Server 还可以作为配置中心，对 Spring Cloud 应用的外部配置进行统一地集中化管理。而我们只需要在应用的 POM 文件中引入 spring-cloud-starter-alibaba-nacos-config 即可实现配置的获取与动态刷新。

从配置管理的角度看，Nacos 可以说是 Spring Cloud Config 的替代方案，但相比后者 Nacos 的使用更简单，操作步骤也更少

##### 动态刷新

@RefreshScope，当配置中心更改配置后，响应的getId值会刷新

~~~java
@RefreshScope
public class IdEntity {
    @Value("${id}")
    private int id;
    public int getId(){
        return this.id;
    }
}
~~~





#### Nacos服务配置三种方案

1. DataId
   - 指定DataId
   - 在web端配置一个config
   - 在ymal中激活对应的环境
2. Group
   - 通过Group实现环境区分
   - 新建group
   - 修改配置文件信息
3. Namespace
   - 新建命名空间
   - 新建DataId

三者关系：![image-20220407230120882](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204072301174.png)



#### 命名空间

用于进行租户粒度的配置隔离，不同命名空间下，可以存在相同的groupId和Data Id的配置。命名空间的常用常见为：为不同环境的配置区分隔离，例如开发、测试、生产等等环境的资源(配置、服务等等)隔离等

一个命名空间下，可存在多个配置，一个配置对应一个DataId

#### GroupId

Nacos中的一组配置集，是组织配置的维度之一，通过一个有意义的字符串对配置集进行分组，从而区分Data Id相同的配置集。在Nacos上创建一个配置时，若未填写分组的名称，则配置分组的名称默认未Default_group

配置分组的常见场景：不同的应用或组件使用了相同的配置类型，如：dataId_url、MQ_topic配置

#### DataId

Nacos中某个配置集的Id。配置集ID是组织划分配置的维度之一。DataId常用于组织划分系统的配置集，一个系统或应用可以配置多个配置集，每个配置集都可以被一个有意义的名称标识

DataId通常采用类Java包的命名规则保证全局唯一性，非强制

Springcloud中，dataId的完整格式如下：

~~~java
${prefix}-${spring.profile.active}.${file-extension}
~~~





#### Nacos配置中心集群

工作原理：

Nacos作为配置中心的集群结构中，是一种无中心化节点的设计，由于没有主从节点，也没有选举机制，所以为了能够实现热备，就需要增加虚拟IP（VIP）。

Nacos的数据存储分为两部分

1. Mysql数据库存储，所有Nacos节点共享同一份数据，数据的副本机制由Mysql本身的主从方案来解决，从而保证数据的可靠性。
2. 每个节点的本地磁盘，会保存一份全量数据，具体路径：`/data/program/nacos-1/data/config-data/${GROUP}`

![](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/1666682-20220329151449681-423821799.png)

当配置发生变化时：

1. Nacos会把变更的配置保存到数据库，然后再写入本地文件。
2. 接着发送一个HTTP请求，给到集群中的其他节点，其他节点收到事件后，从Mysql中dump刚刚写入的数据到本地文件中



#### Nacos Server集群化部署（后期深入）

![image-20220407230202144](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204072302345.png)

三种部署方式：

1. 单机模式：用于调试
2. 集群模式：生产环境
3. 多集群模式：多数据环境



#### Nacos持久化

默认自带嵌入式数据库derby

derby切换到本地库步骤

1. 在nacos安装目录下的conf中找到sql脚本，然后导入本地数据库中
2. 找到application.properties，修改其默认数据库配置信息
3. 重启nacos，这时，使用的就是笨的数据库了



#### Nacos实现原理

##### Nacos架构分析

![image-20220511215652631](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202205112156762.png)

Namer Server：通过VIP或者DNS的方式实现高可用集群的服务路由

Nacos Server：OpenAPI是功能访问入口、Config Server配置服务、Naming Server 名字服务模块、Consistency Protocol一致性协议 采用Raft算法Redis哨兵 Etcd也采用了这种算法

服务提供者通过VIP(Virtual IP)访问Nacos Server好可用集群，基于Open API完成服务的注册和服务的查询。Nacos Server本身可以支持主备模式，所以底层会采用数据一致性算法来完成leaer和follower节点的数据同步。服务消费者也是如此，基于Open API从Nacos Server中查询服务列表



##### 注册中心原理

服务注册的功能体现在：

1. 服务实例启动时注册到服务注册表，并在关闭时注销
2. 服务消费者查询服务注册表，获取可以实例
3. 服务注册中心需要调用服务实例的健康检查API来验证它是否能够处理请求

![image-20220511223904089](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202205112239186.png)

服务注册完整过程

1. Nacos客户端通过OpenAPI的形式发送服务注册请求
2. Nacos收到请求后，做以下三件事
   - 构建一个Service对象保存到ConcurrentHashMap集合中
   - 使用定时任务对当前服务下的所以实例建立心跳检测机制
   - 基于数据一致性协议讲服务数据进行同步



##### 服务提供者地址查询

查询列表形式

1. 基于OpenAPI

   ~~~java
   curl -X GET 127.0.0.1:8848/nacos/v1/ns/instance/list?servicename=xxx
   ~~~

   

2. 使用SDK方式



#### Nacos服务地址动态感知原理

服务消费者不仅需要获得服务提供者的地址列表，还需要在服务实例出现异常时监听服务地址的变化

服务动态感知基本原理：



![image-20220511225415719](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202205112254839.png)

Nacos客户端有一个HostReactor类，他的功能是实现服务的动态更新：

1. 客户端发起事件订阅后，HostReactor中有UpdateTask线程，每10s发送一次Pull请求，获得服务端最新的地址列表
2. 服务端与服务提供者的实例之间维持了心跳检测，一旦服务提供者出现异常，则会发送一个Push消息给Nacos客户端，也就是服务消费者
3. 服务消费者收到请求后，使用HostReactor的processServiceJSON方法解析消息，更新本地服务地址列表

Nacos本地会维护一个存储服务信息的缓存文件，每次客户端从注册中心获取新的服务信息时都会进行本地化的处理，包括更新缓存服务、发布事件、更新本地文件


https://blog.csdn.net/liyanan21/article/details/89088603
心跳：5s一次

查找服务、发送心跳

最终都会调用：nacos提供的SDK





### Sentinel

> 面向分布式微服务架构的轻量级高可用**流量控制组件**。其主要以流量为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性
>
> 从功能上来说：类似于Hystrix，但是比Hystrix更强大

[参考文章](http://c.biancheng.net/springcloud/sentinel.html)

互联网应用中，有很多突发性的高并发访问场景，在没有任何保护的机制下，所以的流量进入服务器，很可能导致整个系统不可用。为了保证系统在这些场景中仍然能稳定运行，就需要采取一定的系统保护策略，常见的保护策略：服务降级、限流、熔断等等



#### 服务限流的作用与实现

限流的目的是：通过限制并发访问数或者限制一个时间窗口内允许处理的请求数量来保护系统，一旦达到限制数量则对当前请求进行处理，采用对应的拒绝策略。

本质上来说限流的主要作用是：损失一部分用户的可用性，为大部分用户提供稳定可靠的服务

要实现限流：最主要的是限流的算法

#### 限流算法

##### 计数器算法

1. 计数器算法是使用计数器在时间周期内累加访问次数，当达到设定的限流值时，触发限流策略。下一个时间周期开始时，进行清零，重新计数
2. 此算法在单机还是分布式环境下实现都非常简单，使用redis的incr原子自增性和线程安全即可轻松实现
3. ![image-20220512214058960](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202205122141144.png)

假设1min内服务器的负载能力为100，因此一个周期的访问量限制在100，然而在第一个周期的最后5秒和下一个周期的开始5秒时间段内，分别涌入100的访问量，虽然没有超过每个周期的限制量，但是整体上10秒内已达到200的访问量，已远远超过服务器的负载能力，由此可见，计数器算法方式限流对于周期比较长的限流，存在很大的弊端

##### 滑动窗口算法

滑动窗口算法是**将时间周期分为N个小周期**，分别记录每个小周期内访问次数，并且根据时间滑动删除过期的小周期。

如下图，假设时间周期为1min，将1min再分为2个小周期，统计每个小周期的访问数量，则可以看到，第一个时间周期内，访问数量为75，第二个时间周期内，访问数量为100，超过100的访问则被限流掉了

![image-20220512214710124](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202205122147235.png)

当滑动窗口的格子划分的越多，那么滑动窗口的滚动就越平滑，限流的统计就会越精确

Sentinel采用了滑动窗口算法来实现限流

##### 令牌桶限流算法

![image-20220512215008778](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202205122150908.png)

令牌桶限流算法在网络流量整形和速率限制(Rate Limiting)中常用的一种算法

令牌桶算法是程序以r（r=时间周期/限流值）的速度向令牌桶中增加令牌

系统会以一个恒定速率往固定容量的令牌桶里加入Token，如果桶已经满了就不再加了，新请求来临时会拿走一个Token,如果没有Token可拿就阻塞或者拒绝服务

例如令牌生成速度是10s/个，也就等同于QPS=10，当请求获取令牌时，会存在以下三种情况

1. 请求速度大于令牌生成速度：令牌会被很快取完，后续再进来的请求会被限流
2. 请求速度等于：此时流量处于平稳状态
3. 请求速度小于：说明此时系统的并发数不高，请求能被正常处理

由于令牌桶有固定的大小，当请求速度小于令牌生成速度时，令牌桶会被填满，所以令牌桶能够处理突发流量，也就是在短时间内新增的流量系统能够正常处理，这是令牌桶的特性

##### 漏桶限流算法

漏桶限流算法的主要作用是控制数据注入网络的速度，平滑网络上的突发流量

在漏桶算法内部同样维护一个容器，这个容器会以恒定速度出水，不管上面的水流速度多快，漏桶水滴的流出速度始终保持不变。访问请求到达时直接放入漏桶，如当前容量已达到上限（限流值），则进行丢弃（触发限流策略）。漏桶以固定的速率进行释放访问请求（即请求通过），直到漏桶为空。

消息中间件就使用了漏桶限流的思想，不管生产者的请求量有多大，消息的处理能力取决于消费者

![image-20220512221233106](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202205122212206.png)



在漏桶限流算法中，存在以下几种可能的情况：

1. 请求速度大于漏桶流出水滴的速度：也就是请求数超出当前服务所能处理的极限，将会触发限流策略
2. 请求速度小于或者等于漏桶流出水滴的速度，也就是服务端的处理能力正好满足客户端的请求量，将正常执行

漏桶限流算法和令牌桶限流算法的实现原理相差不大，最大的区别是漏桶无法处理短时间内的突发流量，漏桶限流算法是一种恒定速度的限流算法。





---

#### Sentinel

是一种面向分布式微服务架构的轻量级高可用流量控制组件，主要以流量为切入点，从流量控制、熔断降级、系统负载保护等多个维度帮助用户保护服务的稳定性

功能上来说，Sentinel 与 Spring Cloud Netfilx Hystrix 类似，但 Sentinel 要比 Hystrix 更加强大，例如 Sentinel 提供了流量控制功能、比 Hystrix 更加完善的实时监控功能等等

#### Sentinel的组成

1. Sentinel核心库(Java客户端)：Sentinel的核心库不依赖任何框架或库，能够运行与Java8及以上的版本的运行时环境，同时对Spring Cloud、Dubbo等微服务框架提供了很好的支持
2. Sentinel控制台(Dashboard)：Sentinel提供了一个轻量级的开源控制台，它为用户提供了机器自发现、簇点链路自发现、监控、规则配置等功能。基于SpringBoot开发，打包后可直接运行，不需要额外Tomcat等应用容器

Sentinel核心库不依赖Sentinel Dashboard，但两者结合使用可以有效的提高效率，让Sentinel发挥它最大的作用

#### Sentinel基本应用

使用Sentinel的核心库来实现限流，主要有以下几个步骤

1. 定义资源
   - 所谓资源，就是通过限流保护的最基本的元素，比如一个方法。有了需要保护的资源之后，就可以针对该资源设置限流控制规则了
2. 定义限流规则
3. 检验规则是否生效



##### 资源定义方式

@SentinelResource注解来定义资源 或者 纯Java代码方式 注解方式推荐

使用时候搜索即可



##### 资源保护规则（限流规则）

Sentinel支持多种保护规则：流量控制规则、熔断降级规则、系统保护规则、来源访问控制规则、热点参数规则

通过FlowRule来定义限流规则，FlowRuleManager.loadRules(rules)来加载规则列表

使用时候官网搜索即可

限流策略：

1. 直接拒绝：默认
2. warm up：预热，让通过的流量缓缓增加，在一定时间内逐渐增加到阈值，给系统一个预热的时间，避免冷系统被压垮
3. 匀速排队

##### Sentinel实现服务熔断

Sentinel实现服务熔断操作和配置类似限流，不同在于限流采用FlowRule，而熔断采用DegradeRule

Sentinel熔断策略：

1. 平均响应时间
2. 异常比例
3. 异常数





---

### Seata

[参考文章](http://c.biancheng.net/springcloud/seata.html)

Seata是一个分布式事务处理框架，阿里巴巴和蚂蚁金服共同开源的分布式事务解决方案，能够在微服务架构下提供高性能且简单易用的分布式事务服务。Seata为用户提供了AT、TCC、SAGA、XA事务模式，为用户打造一站式分布式解决方案

![image-20220408164209711](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/image-20220408164209711.png)



#### 分布式事务相关概念

**事务**：由一组操作构成的可靠、独立的工作单元，事务具备ACID特性，即：原子性、一致性、隔离性、持久性

**本地事务**：本地事务由本地资源管理器(通常指：数据库管理系统，例如MySQL、Oracle等等)管理，严格地支持ACID特性，高效可靠。本地事务不具备分布式事务的处理能力，隔离的最小单位受限于资源管理器，即**本地事务只能对自己数据库的操作进行控制，对于其他数据库的操作则无能为力**

**分支事务**：分布式事务中，就是一个个受全局事务管辖和协调的本地事务

**全局事务**：指的是一次性操作多个资源管理器完成的事务，由一组分支事务组成

分布式事务可以理解为：一个包含了若干个分支事务的全局事务。全局事务的职责是协调其管辖的各个分支事务达成一致，要么一起成功提交，要么一起失败回滚。通常，分支事务本身就是一个满足ACID特性的本地事务









#### Seata整体工作流程

Seata对分布式事务的协调和控制主要通过XID和3个核心组件实现

##### XID

XID是全局事务的唯一标识，它可以在服务的调用链路中传递，绑定到服务的事务上下文中

##### 核心组件

- TC（Transaction Coordinator）:事务协调器，它是事务的协调者，主要 务和分支事务的状态，驱动全局事务提交或回滚
- TM（Transaction Manager)：事务管理器，它是事务的发起者，负责定义全局事务的范围，并根据TC维护的全局事务和分支事务状态，做出开始事务、提交事务、回滚事务的决议
- RM（Resource Manager)：资源管理器，它是资源的管理者（可以理解为各服务使用的数据库）。它负责管理分支事务上的资源，向TC注册分支事务，汇报分支事务状态，驱动分支事务的提交或回滚



以上三个组件相互协作，**TC 以 Seata 服务器（Server）形式独立部署，TM 和 RM 则是以 Seata Client 的形式集成在微服务中运行**，其整体工作流程如下图。

![img](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/img/102A115W-0.png)

 

Seata 的整体工作流程如下：

1. TM 向 TC 申请开启一个全局事务，全局事务创建成功后，TC 会针对这个全局事务生成一个全局唯一的 XID；
2. XID 通过服务的调用链传递到其他服务;
3. RM 向 TC 注册一个分支事务，(TC)并将其纳入 XID 对应全局事务的管辖；
3. RM向TC汇报资源的准备状态
4. TM 根据 TC 收集的各个分支事务的执行结果，向 TC 发起全局事务提交或回滚决议；
5. TC 调度 XID 下管辖的所有分支事务完成提交或回滚操作。



#### Seata AT模式

Seata提供了AT、TCC、SAGA、XA四种事务模式，可以快速有效地对分布式事务进行控制，四种模式中，使用最多、最方便的就是AT模式。与其他事务模式相比，AT模式可以应对大多数的业务场景，且基本可以做到无业务入侵，开发人员有更多的精力关注于业务逻辑开发

##### AT模式的前提

任何应用想要使用Seata的AT模式对分布式事务进行控制，必须满足以下2个前提：

1. 必须使用支持本地ACID事务特性的关系型数据库
2. 应用程序必须是使用JDBC对数据库进行访问的Java应用



##### 实现原理/AT模式的工作机制

两阶段提交协议的演变

1. 第一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源
2. 第二阶段：提交异步化，非常快速地完成。回滚通过一阶段的回滚日志进行反向补偿

###### 第一阶段实现原理

1. 将业务数据在**更新前、后**保存到undo_log日志表中（这个undo_log表并不是mysql的undo log日志文件哈）
2. 利用本地事务的ACID特性，把业务数据的更新和回滚日志写入到同一个本地事务中进行提交
3. ![image-20220420220151103](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204202202173.png)

从原理来看，分支的本地事务可以在第一阶段提交完成后马上释放本地事务锁定的资源



###### 第二阶段实现原理

TC接收到所有事务分支的事务状态汇报之后，决定对全局事务进行提交或回滚

**事务提交分析**：

如果决定是全局提交，说明此时所有分支事务已经完成了提交，只需要清理undo_log表日志即可。由于第一阶段所有的分支事务都已经提交了，所有这里不需要TC来触发所有分支事务的提交

![image-20220420220605423](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204202206568.png)

事务提交的执行流程：

1. 分支事务接收到TC的提交请求后，把请求放到一个异步任务队列中，并且马上返回提交成功的结果给TC
2. 从异步队列中执行分支，提交请求，批量删除相应undo_log日志



**事务回滚分析**

所谓回滚，无非就是对undo_log表中记录的数据前、后镜像进行补偿。全局事务回滚成功，则数据的一致性就得到了保证

![image-20220420221305358](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204202213508.png)

执行流程：所有分支事务接收到TC的事务回滚请求后，分支事务参与者开启一个本地事务，执行以下操作

1. 通过XID(全局ID)和branch ID(分支ID)查找相应的undo_log记录
2. 数据校验：拿UNDO_LOG中的afterImage镜像数据与当前业务表中的数据进行比较，如果不同，说明数据被当前全局事务之外的动作做了修改，那么事务将不会回滚。如果afterImage中的数据和当前业务表中对应的数据相同，则根据UNDO_LOG中的beforeImage镜像数据和业务SQL的相关信息生成回滚语句并执行
3. 提交本地事务，把本地事务的执行结果(即分支事务回滚的结果)上报给TC



##### 事务隔离性的保证

在ACID事务特性中，有一个隔离性，所谓的隔离性是指多个用户并发访问数据库时，数据库为每个用户开启的事务不能被其他事务的操作所干扰，多个并发事务之间要相互隔离

AT模式中，多个全局事务操作同一张表时，它的事务隔离级别是基于全局锁来实现的

###### 写隔离

写隔离是为了在多个全局事务针对同一张表的同一个字段进行更新操作时，避免全局事务在没有被提交之前被其他全局事务修改。

写隔离的主要实现是，在第一阶段本地事务提交之前，确保拿到该记录的全局锁（由TC控制）。如果拿不到全局锁，则不能提交本地事务。并且获取全局锁的尝试会在一个范围内限制，如果超出范围将会放弃全局锁的获取，并且回滚事务，释放本地锁

![image-20220420225532062](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204202255214.png)

如果tx1在第二阶段执行全局回滚，那么tx1需要重新获得该数据的本地锁，然后根据UNDO_LOG进行事务回滚。此时，如果tx2仍然在等待该记录的全局锁，同时持有本地锁，那么tx1分支事务的回滚会失败。tx1分支事务的回滚过程会一直重试，直到tx2的全局锁获取超时，放弃全局锁并回滚本地事务、释放本地锁，之后tx1的分支事务才会回滚成功。而在整个过程中，全局锁在tx1结束之前一直被txl持有，所以不会发生脏写的问题

![image-20220420230803425](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204202308686.png)



###### 读隔离

在数据库本地事务隔离级别为Read Committed或者以上时，Seata AT事务模式的默认全局事务隔离级别是Read Uncommitted。在该隔离级别，所有事务都可以看到其他未提交事务的执行结果，产生脏读。这在最终一致性事务模型中是允许存在的，并且在大部分分布式事务场景中都可以接受脏读

在某些特定场景中要求事务隔离级别必须为Read Committed，目前Seata是通过SelectForUpdateExecutor执行器对SELECT FOR UPDATE语句进行代理的，SELECT FOR UPDATE语句在执行时会申请全局锁。如图所示，如果全局锁已经被其他分支事务持有，则释放本地锁(回滚SELECT FORUPDATE语句的本地执行）并重试。在这个过程中，查询请求会被“BLOCKING”，直到全局锁被拿到，也就是读取的相关数据已提交时才返回

![image-20220420230205383](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204202302521.png)







此外，我们还需要针对业务中涉及的各个数据库表，分别创建一个 UNDO_LOG（回滚日志）表。不同数据库在创建 UNDO_LOG 表时会略有不同，以 MySQL 为例，其 UNDO_LOG 表的创表语句如下：

```mysql
CREATE TABLE `undo_log` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `branch_id` bigint(20) NOT NULL,
  `xid` varchar(100) NOT NULL,
  `context` varchar(128) NOT NULL,
  `rollback_info` longblob NOT NULL,
  `log_status` int(11) NOT NULL,
  `log_created` datetime NOT NULL,
  `log_modified` datetime NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ux_undo_log` (`xid`,`branch_id`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;
```





##### @GlobalTransactional注解

分布式微服务中，Seata提供了@GlobalTransactional注解实现分布式事务的开启、管理和控制

当调用这个注解标记的方法时，TM会先向TC注册全局事务，TC生成一个全局的XID，返回给TM

该注解可以在类上使用，亦可以在方法上使用，该注解的使用位置决定了全局事务的范围

- 在类中某个方法使用时，全局事务的范围就是该方法以及它所涉及的所有服务。
- 在类上使用时，全局事务的范围就是这个类中的所有方法以及这些方法涉及的服务。





#### Seata配置中心

seata支持的配置中心有很多：Nacos、etcd、apollo、zookeeper、consul、file(读本地文件，包括conf、properties、yml等配置文件)

对于Seata来说，Nacos是一种重要的配置中心实现



Seata整合Nacos配置中心：参考文章http://c.biancheng.net/springcloud/seata.html

主要步骤：

1. Seata 服务端配置
2. Seata客户端配置
3. 上传配置到Nacos配置中心
4. 验证Nacos配置中心：登陆Nacos控制台查看配置列表



#### Seata配置 注册中心

Nacos、Eureka、Zookeeper、etcd等

1. 修改Seata服务端配置—修改注册中心类型
2. Seata客户端配置注册中心
3. Nacos中查看启动的seata服务



#### Seata事务组

和seata集群有关了，暂时跳过，知道即可

> 事务分组是Seata提供的一种TC(Seata Server)服务查找机制



#### Seata Server启动

seata server共有3中存储模式

1. file
   - 文件存储模式，默认模式
   - 该模式为单机模式，全局事务的会话信息在内存中读写，并持久化到本地文件root.data，性能高
2. db
   - 数据库存储模式
   - 该模式为高可用模式，全局事务会话信息通过数据库共享，性能较低
   - 该模式下，会建立针对全局事务会话信息的相关表
     - 全局事务表：global_table
     - 分支事务表：branch_table
     - 全局锁表：lock_table
3. redis
   - 缓存存储模式
   - 1.3及以上版本支持，性能高，但存在事务信息丢失风险







# Dubbo

> 阿里开源的基于Java的高性能RPC分布式服务框架，一款微服务开发框架
>
> **它提供了RPC通信和微服务治理两大关键功能**



基本原理架构及调用关系：

![image-20220405225319484](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204052253144.png)

1. 将服务转载容器中，服务容器负责启动、加载、运行服务提供者
2. 服务提供者在启动时，向注册中心注册自己提供的服务
3. 服务消费者在启动时，在注册中心订阅自己所需的服务
4. 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者
5. 消费者从提供者列表中，基于软负载均衡算法，选择一台提供者进行调用，如果调用失败会再选另一台进行调用
6. 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心





### 为什么要用Dubbo

1. 内部使用了netty来进行通信、zookeeper保证了高性能可用性
1. 使用Dubbo可以将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，可用于提高业务复用灵活扩展，使前端应用能更快速的响应多变的市场需求。
2. 分布式架构可以承受更大规模的并发流量



### Dubbo和SpringCloud区别

1. 通信方式不同
   - Dubbo 使用的是 RPC 通信，而Spring Cloud 使用的是HTTP RESTFul 方式
   
2. 组件组成不同
   - dubbo的服务注册中心为Zookeerper，服务监控中心为dubbo-monitor，无消息总线、服务跟踪、批量任务等组件
   - springcloud的服务注册中心为eureka、服务监控中心为springboot admin，有消息总线，数据流、服务跟踪、批量任务等组件
   
   

### Dubbo需要Web容器吗

不依赖容器，如果硬要用Web容器，只会增加复杂性，也浪费资源





### Dubbo内置的服务容器

1. Spring Container
2. Jetty Container
3. Log4j Container

Dubbo的服务容器只是一个简单的Main方法，并加载一个简单的Spring容器，用于暴露服务

---



### Dubbo支持的协议

1. dubbo://（推荐）
1. rmi://
1. hessian://
2. http://
2. webservice://
2. thrift://
3. rest://
4. redis://
5. memcached://

---



### Dubbo里面的几种节点角色

1. provide：暴露服务的服务提供方
2. consumer：调用远程服务的服务消费方
3. registry：服务注册与发现的注册中心
4. monitor：统计服务调用次数和调用时间的监控中心
5. container：服务运行容器

---



### Dubbo服务注册与发现

Dubbo基于消费者端的自动服务发现能力，其基本原理如图：

![image-20220405234944103](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204052349243.png)

#### Dubbo默认注册中心

> Dubbo推荐使用Zookeeper作为注册中心，还有redis、multicast、simple等注册中心



服务发现的的一个核心组件是注册中心，Provider 注册地址到注册中心，Consumer 从注册中心读取和订阅 Provider 地址列表。 因此，要启用服务发现，需要为 Dubbo 增加注册中心配置：

~~~yaml
dubbo:
  registry:
    address: 
      zookeeper: //127.0.0.1:2181
~~~





### 服务流量管理

Dubbo通过定义路由规则，实现对流量分布的控制，结合Sentinel（类似与Hystrix)

#### 流量管理

流量管理本质是将请求根据定制好的路由规则分发到应用服务上

![image-20220406100247529](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/image-20220406100247529.png)



- 路由规则可以有多个，不同的路由规则之间存在优先级。如：Router(1) -> Router(2) -> …… -> Router(n)
- 一个路由规则可以路由到多个不同的应用服务。如：Router(2)既可以路由到Service(1)也可以路由到Service(2)
- 多个不同的路由规则可以路由到同一个应用服务。如：Router(1)和Router(2)都可以路由到Service(2)
- 路由规则也可以不路由到任何应用服务。如：Router(m)没有路由到任何一个Service上，所有命中Router(m)的请求都会因为没有对应的应用服务处理而导致报错
- 应用服务可以是单个的实例，也可以是一个应用集群





---

### Dubbo部署架构——三大中心化组件

这三个中心并不是运行Dubbo的必要条件，用户可以根据自身业务情况决定启用一个或多个，以达到简化部署的目的。通常情况下，所有用户都会以独立的注册中心 开始 Dubbo 服务开发，而**配置中心、元数据中心则会在微服务演进的过程中逐步的按需被引入进来**

Dubbo微服务组件与各个中心交互：

![image-20220406102116248](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/image-20220406102116248.png)

#### 注册中心

协调Consumer和Provider之间的地址注册与发现，它承载着服务注册和服务发现的职责。目前Dubbo支持两种粒度的服务注册和发现，分别是接口级别、应用级别，注册中心可以按需进行部署



#### 配置中心

1. 存储Dubbo启动阶段的全局配置，保证配置的跨环境共享与全局一致性
2. 负责服务治理规则(路由规则、动态配置等等)的存储与推送

#### 元数据中心

1. 接收Provider上报的服务接口元数据，为Admin等控制台提供运维能力(如：服务测试、接口文档等等)
2. 作为服务发现机制的补充，提供额外的接口/方法级别配置信息的同步能力，相当于注册中心的额外配置

---



### Dubbo服务间调用是阻塞的吗

> 默认是同步等待结果阻塞的，支持异步调用



Dubbo是基于NIO的非阻塞实现并行调用，客户端不需要启动多线程即可完成并行调用多个远程服务，相对多线程开销较小，异步调用会返回一个Future对象

---



### Dubbo序列化框架

默认使用Hessian序列化，还有Duddo、FastJson、Java自带序列化







### Dubbo核心配置

![image-20220405234317683](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202204052343833.png)

---



### 在Provider上可以配置Consumer端的属性有哪些

1. timeout：方法调用超时
2. retries：失败重试次数，默认重试2次
3. loadbalance：负载均衡算法，默认随机
4. actives：消费者端最大并发调用限制

---



### Dubbo负载均衡策略

1. random loadbalance：权重设置随机概率，默认负载均衡策略
2. rounddrobin loadbalance：轮询
3. lastactive loadbalance：最少活跃调用数，若相同则随机
4. 4、consistenthash loadbalance：一致性hash，相同参数的请求总是发送到同一提供者

---



### Dubbo网络通信框架

Dubbo 默认使用 Netty 框架作为网络通信，也是推荐的选择，另外内容还集成有Mina、Grizzly

netty是基于NIO的，它封装了JDK的NIO，使用起来更方便

---



### Netty



#### NIO/BIO

BIO是面向流(字节/字符流)同步阻塞的

NIO是面向缓冲区(或面向块)和非阻塞的，数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区前后移动

#### NIO非阻塞模式

1. 非阻塞读：一个线程从某通道读取数据时，仅能读到目前准备好的数据，如果数据还没准备好就立即返回，该线程可以继续做别的事情，而不是像BIO那样保持线程阻塞
2. 非阻塞写：一个线程写入一些数据到某通道时，先将数据写到缓冲区，等到数据可写时，再将缓冲区的数据写到通道。而BIO调用了write之后就阻塞在这里，即使数据不可写也要一直等待，直到数据完全写出去

##### NIO三大组件

1. IO多路复用器selector
2. 基于缓冲区的双向管道，channel和buffer

![image-20220406113048060](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/image-20220406113048060.png)

NIO是面向缓冲和非阻塞的，在一个NIO中一个线程处理selector，一个selector可以处理多个channel。使用selector时，必须将channel注册到selector中

##### Buffer

Buffer本质是一个内存块，基于数组实现，Buffer是一个抽象类，7个基本类型(除了boolean)都有自己的实现类，常用的为ByteBuffer，因为网络数据都是以字节方式传输的

buffer读模式和写模式。既可以读又可以写，但是读写要做切换，也就是说，如果buffer当前模式为写模式，则要显式切换到读模式才可以读数据，反之亦然

// 调用flip方法  buffer.flip();   切换到读模式

##### Channel

常见的几种channel

FileChannel

DatagramChannel(用于udp)

ServerSocketChannel(用于TCP)

SocketChannel(用于TCP)





---



### Dubbo集群容错方案

1. Failover Cluster：失败自动切换，自动重试其他服务器，默认方式
2. Failfast Cluster：快速失败，立即报错，只发起一次调用
3. Failsafe Cluster：失败安全，出现异常时直接忽略
4. Failback Cluster：失败自动恢复，记录失败请求，定时重发
5. Forking Cluster：并行调用多个服务器，只要一个成功即返回
6. Broadcast Cluster：广播逐个调用所有提供者，任意一个报错则报错



### Dubbo分布式事务

主要还是利用自家产品：Seata实现分布式事务





----------------

# Nginx

### 与SpringCloud gateway区别

1. gateway是前端到后端服务器之间的对内网关，nginx是用户到前端工程的对外网关

### Nginx在微服务中的地位

![image-20221121232654871](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202211212326914.png)

外部请求首先经过Nginx层进行代理，代理分发到网关服务，网关服务在分发请求给具体的机器



### 正向代理

一般是客户端直接向目标服务器发送请求并获取内容，使用正向代理后，客户端改为向代理服务器发送请求，并且指定目标服务器，然后由代理服务器和原始服务器（目标服务器）进行通信，转交请求并获取内容，再返回给客户端

正向代理隐藏了真实的客户端，为客户端收发请求，使真实客户端对目标服务器不可见

例子：国内访问google等网站，这个时候需要一个代理服务器来帮助访问google，代理服务器与google进行通信，并返回数据

### 反向代理

反向代理指的是：使用反向代理后，代理服务器接收网络上的请求，然后将请求转发给内部网络上真正进行处理的服务器，并将服务器上得到的结果返回给网络上连接的客户端，此时代理服务器对外就表现为一个反向代理服务器

反向代理隐藏了真实处理的服务器，为服务器收发请求，使得真实处理的服务器对客户端不可见 



![image-20221122095905315](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/springcloud_img/202211220959406.png)



### 负载均衡

Nginx实现负载均衡，一般来说是将请求**转发给服务器集群**









# 分布式ID

分布式ID需要满足哪些要求

1. 全局唯一
2. 方便易用
3. 高可用：可用性要保证无限接近于100%
4. 高性能：生成id要快、对本地资源消耗要小
5. 有序递增：如果id要存数据库，id的有序性可以提升数据库写入速度，并且很多时候可能根据id来排序
6. 安全：Id不能包含敏感信息
7. 可独立部署：即有一个单独的法号器范围，专门用来生成ID



全局唯一ID实现方式

- 数据库唯一id
- 基于数据库号段模式
  - 第一种每次获取id需要访问一次数据库，这明显是不行的
  - 批量获取，然后存在内存，用到的时候直接内存拿来用，这就是基于数据库的号段模式来生成分布式ID
  - 
- UUID
  - 生成速度快、简单易用
  - 存储空间大（32个字符串，128位），可能产生重复
- NoSQL的Redis
  - incr命令实现对id原子顺序递增
- 雪花算法(常用)
- 开源框架：百度的UidGenerator基于雪花、美团Leaf、滴滴Tinyid、

# 分布式锁

- redis分布式锁：set nx px  和RedLock机制
- zookeeper分布式锁
- 数据库(很少用)



# 分布式事务

数据库ACID

A：Atomicity，原子性

C：Consistency，一致性：事务执行前后，数据保持一致

I：Isolation，隔离性

D：Durabilily，持久性

MySQL InnoDB使用

- redo log保证持久性
- undo log保证事务原子性
- 锁机制、MVCC保证隔离性

A、I、D是手段，C才是目的



场景：

- 微服务架构下，一个系统被拆分为多个小的服务
- 每个服务可能部署在不同的集群上，并且每个服务可能使用单独的数据库
- 这种情况下，一个操作就可能涉及多个微服务及多个数据库

问题：如何保证这一个操作，要么都成功，要么都失败

这个时候，单单依靠数据库就不能完成这个任务了。需要引入分布式事务

实际上，只要存在跨库，都需要引入分布式数据库了

分布式事务的终极目标

- ==保证系统中多个相关联的数据库中的数据的一致性==



分布式事务也属于事务，理论上就应该遵守ACID特性。

- 但是，考虑到性能、可用性等方面，往往无法做到ACID，只能选择一个折中的方案

## 分布式事务理论基础

### CAP、BASE理论

这个理论不只是分布式事务的基础，而是整个分布式架构的基础



### 柔性事务

互联网应用，最关键的就是保证高可用

在此场景下，一些大佬在cap、base理论基础上提测了柔性事务。柔性事务追求最终一致性

柔性事务就是：base+业务实践

- 根据我们自身业务特性，通过适当的方式来保证系统数据的最终一致性

TCC、Sage、MQ事务、本地消息表 这些都属于柔性事务

### 刚性事务

刚性事务追求的目标：强一致性

2PC、3PC就是属于刚性事务

## 分布式事务解决方案





# 分布式会话

- 现在都用JWT来解决了



# 分布式任务

1. Quartz
2. XXL-Job
3. Elastic-Job





# 分布式文件存储系统

1. 谷歌的：Googel File System ：GFS
2. 阿里的：Taobao File System ：TFS
3. Hadoop Distributed File System ：HDFS
4. 淘宝的：FastDFS
5. MinIO、MogileFs、GridFs等等一系列



# 分布式计算

Hadoop、Storm、Spark、

