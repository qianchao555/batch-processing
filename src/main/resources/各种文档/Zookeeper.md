# Zookeeper

zookeeper是一个开源的==分布式协调服务==

设计目的：将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用

**主要用来解决分布式集群中应用系统的一致性的问题**

zooKeeper 本质上是一个分布式的小文件存储系统。提供基于类似于文件系统的目录树方式的数据存储，并且可以对树种 的节点进行有效管理。从而来维护和监控你存储的数据的状态变化。将通过监控这些数据状态的变化，从而可以达到基于数据的集群管理。诸如：统一命名服务（dubbo）、分布式配置管理（solr的配置集中管理）、分布式消息队列（sub/pub）、分布式锁、分布式协调等功能



很多顶级开源项目都使用到了zk

- Kafka：为Kafka提供Broker和Topic的注册，以及多个Partition的负载均衡等功能。不过Kafka2.8后，引入了基于Raft协议的KRaft模式，不再依赖zk，简化了kafka架构
- Hbase
- Hadoop：zk为Namenode提供高可用支持



## ZK的特点

1. 顺序一致性
   - 从同一个客户端发起的事务请求，最终将会严格按照顺序被应用到zk中去
2. 原子性
   - 所有事务请求的结果，在整个集群所有机器上的情况是一致的。
   - 也就是说，整个集群中所有的机器要么成功完成了某一个事务，要么都没有成功
3. 单一系统映像
   - 无论客户端连接到哪一台zk服务器，其客户端看到的服务器数据模型都是一致的
4. 可靠性
   - 一旦一次更改请求完成，更改的结果就会被持久化
5. 实时性
   - 一旦数据发生变更，其他节点也会感知
6. 集群部署
   - 3~5台就可以组成一个集群，每台机器在内存都保存了zk的全部数据，机器之间互相通信同步数据，客户端连接任何一台机器都可以
7. 高可用
   - 集群中，挂掉不超过半数的集群，都能保证集群可以





## ZK应用场景

zk可以做：发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁、分布式队列等场景



典型应用场景

1. 命名服务：可以通过zk的==树状结构==的特性来命名
2. 分布式锁：通过创建唯一节点获得分布式锁，分布式锁的实现需要用到zk的Watcher机制
   - zk的分布式锁性能方面相对较差，对性能要求很高的话，zk可能就不太适合了
3. 集群管理和注册中心
   - 基本上都是通过创建临时节点+watcher就可以实现 

这些功能的实现，都基本得益于zk可以保存数据的功能，但是zk不适合保存大量数据

CAP中：zk是保证一致性的，Eureka是保证可用性的



## zk重要概念

### Data model(数据模型)

zk数据模型采用层次化的多叉树节点结构。

每个节点都可以存储数据，这些数据可以是数字、字符串、二进制序列。

并且，每个节点还可以拥有N个子节点，最上层是根节点"/"。每个数据节点在zk中，被称为：==znode==。znode是zk中数据的最小单元，每个znode拥有一个唯一的路径标识

> [!WARNING]
>
> 注意：zk主要是用来协调服务的，而不是用来存储业务数据的，所有不要把较大的数据存储在znode上
>
> zk给每个znode数据大小上限是1M





### znode

数据节点，通常分为四大类

- ==持久节点==：一旦创建就一直存在，即使zk集群宕机，直到将其删除
- ==临时节点==：临时节点的生命周期是与客户端会话绑定的，会话消失则节点消失，并且临时节点只能作为叶子节点，不能创建子节点
- ==持久顺序节点==：除了有持久性特性外，子节点的名称还具有顺序性。例如：/node1/app001、/node1/app002
- ==临时顺序节点==：同理



节点组成：每个znode由2部分组成

- stat：状态信息
- data：节点存放的具体数据



Stat类包含了一个数据节点的所有状态信息

| znode状态信息  | 解释                                                         |
| -------------- | ------------------------------------------------------------ |
| cZxid          | create zxid，即该数据节点被创建时的事务id                    |
| ctime          | create time，创建该节点的时间                                |
| mZxid          | modified zxid，该节点最终一次更新时的事务id                  |
| mtime          | modified time，该节点最后一次的更新时间                      |
| pZxid          | 该节点的子节点列表最后一次修改时的事务 id，只有子节点列表变更才会更新 pZxid，子节点内容变更不会更新 |
| cversion       | 子节点版本号，当前节点的子节点每次变化时值+1                 |
| dataVersion    | 当前节点数据内容的版本号，创建时为0，没更新一次版本号值+1    |
| aclVersion     | 节点的ACL版本号，表示该节点ACL信息变更次数                   |
| ephemeralOwner | ephemeral短暂的，创建该临时节点的会话的sessionId，如果当前节点为持久节点，则ephemeral=0 |
| dataLength     | 当前节点数据内容的长度                                       |
| numChildren    | 当前节点的子节点数                                           |

​	

#### ACL权限控制

zk采用AccessControlLists策略来进行权限控制，类似Unix文件系统的权限控制

对于znode的操作权限，zk提供了5种

1. create：能创建子节点
2. read：获取节点数据和列出其子节点
3. write：能设置、更新节点数据
4. delete：能删除子节点
5. admin：能设置节点的权限

其中，create、delete是针对子节点的权限控制

对于身份认证，提供了以下几种

1. world：默认方式，所有用户都可无条件访问
2. auth：不使用任何id，代表任何已认证的用户
3. digest：采用用户名/密码认证方式
4. ip：针对指定ip进行限制



### Watcher(事件监听器)

watcher是zk的一个重要的特性。==zk允许用户在指定节点上注册一些watcher==，并且在一些特定事件触发的时候，zk服务端将事件通知到感兴趣的客户端上去

==该机制是zk实现分布式协调服务的重要特性==

用到zk的地方，基本都离不开watcher机制特性





### 会话（Session）

Session可以看作zk服务器与客户端的一个Tcp长连接

通过这个连接，客户端可以通过心跳检测与服务器保持有效的会话，也能够向zk服务器发送请求并接受响应，同时也能通过该连接接收来自服务器的watcher事件通知





## zk集群

为了保证高可用，zk可以通过集群部署，只要超过半数的zk可以，那么zk集群是可以正常使用的

zk server之间是互相保持着通信的。集群间通过ZAB（Zookeeper Atomic Broadcast)协议通信来保持数据的一致性



zk最典型的集群模式：Master/Slave模式，即主备模式

> 这种模式下，通常master服务器作为主服务器提供写服务，其他Slave服务器通过异步复制的方式获取Master服务器最新的数据提供读服务

但是，zk没有裁采用传统的Master/Slave概念，而是引入了Leader、Follower、Observe三种角色



zk集群中的所有机器通过一个==”Leader选举过程“==，来选出一台被称为”Leader“的机器。Leader可以提供读写访问

除了Leader外，Follower和Observer都只能提供读服务

Followe和Observer的唯一区别：Observer机器不参与Leader的选举过程，也不参与写操作的”过半写成功“策略，因此Observer机器可以在不影响写性能的情况下提升集群的读性能

Observer为zk3.3版本开始新增的角色



### zk集群Leader选举过程

当Leader服务器出现网络中断、崩溃退出、重启等异常情况时，就会触发Leader的选举，这个过程会选举出一个新的Leader服务器

大致流程：

1. Leader election(选举阶段)
   - 节点一开始都处于选举阶段，只要有一个节点得到超过半数节点的投票，那么它就当选Leader
2. Discover(发现阶段)
   - 这个节点，follower与准Leader通信，同步followers最近接收的事务提议
3. Synchronization(同步阶段)
   - 利用leader前一阶段获取的最新提议历史，同步集群中所有的副本，同步完之后，准leader才会成为正在的Leader
4. Broadcast(广播阶段)
   - zk集群正式对外提供服务，并且Leader进行消息广播。同时如果有新节点加入，还需要对新节点进行同步



### zk集群为啥最好是基数台

zk集群中，宕掉几个zk服务器后，==剩下的zk服务器个数 > 宕机的个数，那么整个zk集群依然可用==

例如

- 3台集群，最大允许1台宕机，4台集群依然最大只允许1台宕机
- 5台集群，最大允许2台宕机，6台集群依然最大允许2台宕机

所以，不必要多增加一台zk





### zk集群选举的过半机制 防止脑裂

集群脑裂？

> 一个集群，通常部署在不同的机房，来提高集群的可用性
>
> 保证可用性的同时，会出现一种机房间网络线路故障，导致机房之间网络不通，进而集群被割裂成几个小的集群
>
> 这时候，子集群各自选举Leader，导致”脑裂“的情况

若没有过半机制，当机房之间的网络恢复后，会发现出现多个Leader。仿佛是1个大脑分散成了多个大脑，这就是：脑裂现象

过半机制如何防止的？

- 因为少于等于一半，是不可能产生Leader的，所以不会出现脑裂现象





## ZAB协议与Paxos算法

Paxos算法应该可以说是zookeeper的灵魂了

但是zk并没有完全采用Paxos算法，而是使用ZAB协议作为其保证==数据一致性的核心算法==

ZAB协议并不是像Paxos算法是一种通用的分布式一致性算法，而是一种特别为zk设计的崩溃可恢复的原子消息广播算法



### Paxos算法

> 1990年提出的，一种分布式系统共识算法，第一个被证明完备的共识算法（前提是：不存在拜占庭将军问题，即没有恶意节点的前提下）



共识算法的作用

- 让分布式系统中的多个节点之间，对某个提案达成一致的看法
- 提案的含义：分布式系统中十分广泛，例如：哪一个节点是Leader、多个事件发生的顺序等等都可以是一个提案



由于Paxos算法被国际公认难以理解和实现，不断有人尝试简化这一算法

2013年，才诞生了一个比Paxos算法更加易于理解和实现的共识算法：Raft算法



针对没有恶意节点的情况

- 除了Raft算法外，常用的一些共识算法：ZAB协议、Fast Paxos算法都基于Paxos改进的

针对存在恶意节点的情况

- 一般是使用：工作量证明（POW)、权益证明(PoS)等共识算法
- 这类共识算法的典型应用就是：区块链
- 区块链系统使用的共识算法需要解决的核心问题：拜占庭将军问题。这与我们平时接触的zk、Etcd、Consul等分布式中间件不太一样



Paxos的2个部分

#### Basic Paxos算法

Basic Paxos中存在的三个重要角色

1. 提倡者/协调者
   - 提倡者或者协调者负责接受客户端的请求，并发起提案。提案信息通常包含提案编号和提议的值
2. 接收者/投票员
   - Accptor/Voter，负责对提议者的提案进行投票，同时需要记住自己的投票历史
3. 学习者
   - Learner，如果有超过半数的接收者/投票者就某个提议达成了共识，那么学习者就需要接受这个提议，并就该提议做出运算，然后将运算结果返回给客户端

#### Multi-Paxos思想

Basic Paxos算法仅能对单个值达成共识，为了能对一系列的值达成共识，提出了Multi-Paxos思想

这种思想的核心就是通过多个Basic Paxos实例，就一系列值达成共识，就是多执行几次Basic Paxos



## Raft算法

共识是可容错系统中的一个基本问题：即使面对故障，服务器也可以在共享状态上达成一致





















## zk vs ETCD



ETCD是一种==强一致性的==分布式键值存储，它提供了一种可靠的方式来存储需要由分布式系统或机器集群访问的数据

ETCD内部采用Raft算法，作为强一致性算法，ETCD是基于Go实现的

总的来说，ETCD比zk更优秀一些，而且基本能覆盖zk的所有场景，实现对其的替代





## zk总结

1. zk自身就是一个分布式程序，只要半数以上节点存活，zk集群就能正常提供服务
2. zk将数据保存在内存中，这就保证了高吞吐量和低延迟，但是也限制了每个znode的存储数据率较小
3. zk底层其实只提供了2个功能
   - 管理用户程序提交的数据
   - 为用户提出数据节点监听服务













