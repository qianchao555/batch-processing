## 分库分表

文章都以MySQL来分析



### 什么是分库分表

分库：将原来的一个数据库拆分为多个数据库，部署到不同的机器上

![image-20230319161434888](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/sharding_img/202303191614136.png)



分表：将原来的一张表，拆分为多张表

![image-20230319161535555](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/sharding_img/202303191615698.png)





### 为什么要分库

> 通常按照前辈的经验来讲，MySQL单库支持的并发量在2000，最好保持在1000。如果有2w个并发量的请求，这时候就需要扩容了
>
> 可以将一个库的数据拆分到多个数据库，访问的时候根据一定的规则访问单库，缓解单库的性能压力



数据库主要有这几方面的瓶颈：

1. 磁盘存储
   - 业务量剧增，导致MySQL单机磁盘容量暴增，一直下去会撑爆
   - 所以，需要拆分多个数据库，减少磁盘的使用率
2. 并发连接支撑
   - 高并发场景下，大量的请求数据库，MySQL是支撑不住的
   - 微服务架构出现，就是为了应对高并发。
   - 例如：它把**订单、用户、商品**等不同模块，拆分成多个应用，并且把单个数据库也拆分成多个不同功能模块的数据库（**订单库、用户库、商品库**），以分担读写压力



### 为什么要分表

> 单表数据量太大的话，SQL的查询就会很慢。MySQL在千万级的单表中如果没有命中索引，可能也会拖垮数据库
>
> 即使SQL命中了索引，单表超过千万级，查询也是很慢的。因为MySQL的索引是B+树 结构，千万级别的B+tree，其树的高度会增加一层，也就会多进行IO操作，查询自然会更慢





常见的优化就是：消息队列来削峰+数据库的分库





![image-20220509213552417](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/sharding_img/202205092136558.png)



### 如何进行分库分表

> 数据库分布式核心无非就是数据切分、切分后对数据的定位、整合
>
> 数据切分：将数据分散存储到多个数据库中，使得单一数据库中的数据量变小。通过扩充主机的数量来缓解单一数据库的性能问题



1. 垂直拆分
   - 垂直分库
   - 垂直分表
2. 水平拆分
   - 水平分库
   - 水平分表



#### 垂直分库

将原来的单数据库分为多个数据库

例如：原来的但数据库中有用户表、商品表、订单表

现在拆分为3个数据库：用户库里面存放用户表、商品库存储商品表、订单库存放订单表，把这三个库部署到不同的机器上，可以很好的应对高并发场景



#### 垂直分表

> 若单表中包含几十列甚至上百列，那么管理起来就很混乱，select * 也会很占用I/O 资源。
>
> 这时候，可以将一些不常用的、数据较大的、长度较长的列拆分到另一张表中

例如：一张用户表，可以拆分为：用户基本信息表和用户详细信息表



#### 水平分库

> 水平分库：将表中的数据切分到不同的数据库实例上，每个服务器具有相同的库和表，知识表中的数据集合不一样而已
>
> 它可以有效的缓解单表单库的性能瓶颈和压力

例如：用户库水平拆分为用户库01、用户库02。用户库01里面的用户表记录一些数据，用户库02里面的数据表记录一些数据



#### 水平分表

单表数据量很大时，可以按照某些规则，将数据切分到多张表中去

例如：订单表，可以按照订单的月份拆分，每一个月的订单数据放到一张表中去



#### 水平分库分表策略

1. range范围
2. hash取模
3. range+hash

##### range

范围策略划分表

例如：可以按照表的主键，将0-1000万的划分到一个表_01，1000万-2000万的数据划分到表 _02中.....；可以按照月份拆分；实际情况根据业务而定！



range分表策略的优点：利于扩容、不需要数据迁移。假设表增加到5000w，只需要再水平扩容一张表就好了，之前的0-4000w的数据不需要迁移

range分表策略的缺点：**存在热点问题。**因为订单id是一直在增大的（假设主键自增），也就是说最近一段时间都是汇聚在一张表里面的。比如最近一个月的订单都在`1000万~2000`万之间，平时用户一般都查最近一个月的订单比较多，请求都打到`order_1`表啦，这就导致表的**数据热点**问题。所以分表策略一定要做好！！！



##### hash

> 按照指定的路由key(一般是采用id)，对分表总数进行取模，把数据分散到各个表中

比如：对之前的订单表进行拆分成4个小表，id=5那条数据则放到5 mod 4=1，则放到第三个小表中

hash划分表优点：不会存在热点数据问题

hash划分表缺点：因为是先确定4个小表了，若未来某个时候，表的数据量又到达瓶颈了，需要扩容时，这个问题就比较棘手了。比如现在又扩容了4张表，总共就8个小表了，之前id=5的数据放在第3张表中，现在5 mod 8=5，则，现在需要放到第5张表中，而不是之前的第1张表了。即：需要对历史数据做迁移重新分布



##### range+hash混合

既然range存在热点问题、hash存在扩容迁移问题，那么将两个方案综合起来，取长补短

比如：

1. 先利用range范围方案进行水平分库操作
   - id为0-8000w的数据放到订单库01
   - 8000w-1.5亿的数据放到订单库02
   - 将来扩容时候，把新的数据划分到订单库01.。。。。
2. 库内的表，再利用hash分表



### 什么时候考虑分库分表？

1. 什么时候分库
   - 业务发展很快，还是多个服务共享一个单体数据库，数据库成为了性能瓶颈，就需要考虑分库了
   - 比如订单、用户等，都可以抽取出来，新搞个应用（其实就是微服务思想），并且拆分数据库（订单库、用户库）。
2. 什么时候分表
   - 如果每天的业务增长新增几十万条数据，并且，该表的查询效率变慢时候。考虑分表了
   - 业界流传MySQL单表数据量500w就要考虑了



### 分库分表后导致的问题

分库分表后，会出现一系列问题，下面列举一些

1. 事务问题
   - 假设两个表在不同的数据库，那么本地事务就失效了
   - 分布式事务来解决
2. 跨库关联
   - 可以分两次查询实现，即：将两次查询后的结果，按照我们需要的数据组合起来即可
   - MySQL也支持多个库中不同的表进行关联，指定：库名.表名即可
3. 排序问题
   - 跨节点的count、order by、group by以及聚合函数等问题：可以分别在各个节点上得到结果后，在应用程序端进行合并
4. 分页问题
   - 各个节点查询出结果后，应用程序端(我们写的代码)里在聚合分页
5. 分布式ID
   - 数据被切分后，不能在依赖数据库自身的主键生成机制了，容易造成主键相同的情况
   - 考虑：UUID、雪花生成算法
   - 此外：一般设计数据库时候，数据的id一般不做业务处理！！即：每一条数据除了id外，还可以有一个唯一字段来标识的方式来解决









### 分库分表中间件

>  常用sharding-jdbc(当当开源 jdbc层面)、cobar(阿里开源)、Mycat(基于cobar)、TDDL(淘宝的)、Atlas(360的)

1. sharding-jdbc

   - 这种client层方案的优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合sharding-jdbc的依赖

2. Mycat

   - 这种proxy层方案的缺点在于需要部署，自己及运维一套中间件，运维成本高，但是好处在于对于各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞就行

   

   

   

### 如何让系统从未分库分表动态切换到分库分表上？

前提：分库、分表、中间件那些都准备好之后



生产环境如何动态的切换到分库分表

方案一：停机迁移方案，比较low，给一个时间公告xxx时间段停止服务。。。。



![image-20220509231915146](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/sharding_img/202205092319268.png)

 

方案二：不停机双写方案

这个是常用的一种迁移方案，比较靠谱一些，不用停机



简单来说，就是在线上系统里面，之前所有写库的地方，增删改操作，都除了对老库增删改，都加上对新库的增删改，这就是所谓双写，同时写俩库，老库和新库。



然后系统部署之后，新库数据差太远，用之前说的导数工具，跑起来读老库数据写新库，写的时候要根据modified这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写。

接着导完一轮之后，有可能数据还是存在不一致，那么就程序自动做一轮校验，比对新老库每个表的每条数据，接着如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表的数据都完全一致为止。

接着当数据完全一致了，就ok了，基于仅仅使用分库分表的最新代码，重新部署一次，不就仅仅基于分库分表在操作了么，还没有几个小时的停机时间，很稳。所以现在基本玩儿数据迁移之类的，都是这么干了

![image-20220509235656289](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/sharding_img/202205092356401.png)

---



### 如何设计动态扩容的分库分表方案

> 为什么会出现动态扩容缩容的分库分表方案？
>
> 其实就是我们对服务的发展做一定的评估，根据吞吐量来计算要求的数据库
>
> 例如：
>
> 1. 假设单个MySQL服务器支持3000的并发，预计达到1w的并发就设计4个库；
> 2. 根据数据量大小计算表数据：加入单表存放1000w数据，预计有5000w数据，那么就分5个表

一般设计到扩容数据库服务器就设计到DBA来操作了，这块可以不怎么了解



---

### 分库分表后，id主键如何处理

分库分表后，id主键问题是必然要面对的问题，这个id怎么生成？若根据每个表的自增主键，那肯定不对，每个表的id可能会相同，所以需要一个全局的id来支撑

如下为id重复的情况：

![image-20220510220437156](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/sharding_img/202205102204992.png)

1. 数据库自增id
   - 就是说你的系统里每次得到一个id，都是往一个库的一个表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个id。拿到这个id之后再往对应的分库分表里去写入
   - 好处：方便简单
   - 缺点：单库生成自增id，在高并发场景下会有瓶颈
2. uuid
   - 好处：本地生成不用基于数据库
   - 缺点：**uuid是一个无序的字符串，太长了，不适合作为主键**
3. 系统当前时间
   - 存在问题：高并发场景下，比如一秒几千并发，会有重复的情况，所以基本不考虑使用
   - 适合场景：使用该时间与业务字段拼接作为id场景
4. snowflake算法(雪花算法)





twitter开源的分布式id生成算法，就是一个64位的long型id

例如：

0 | 0001100 10100010 10111110 10001001 01011100 00 | 10001  |  11001  | 0000 00000000



其中各个块的含义：

1. 1bit：第一位为0，代表正数
2. 41 bit：存储毫秒级时间戳，41 bit可以表示的数字多达2^41 - 1，也就是可以表示2 ^ 41 - 1个毫秒值，换算成年就是2^41 /1000606024365  = 69年的时间
3. 10 bit：工作机器id，代表的是这个服务最多可以部署在2^10台机器上，也就是1024台机器。
   - 5bit：机房id（dataceterId)，最多代表2 ^ 5个机房（32个机房）
   - 5bit：机器id(workerId)，每个机房里可以代表2 ^ 5个机器（32台机器）
4. 12 bit：存储序列号，同一毫秒时间戳时，通过这个递增的序列化来区分。这个是用来记录同一个毫秒内产生的不同id，12 bit可以代表的最大正整数是2 ^ 12 - 1 = 4096，也就是说可以用这个12bit代表的数字来区分同一个毫秒内的4096个不同的id



可以将雪花算法作为一个单独的服务进行部署或公共的方法即可，需要全局唯一id的系统，请求雪花服务获取Id即可。



对于每一个雪花算法服务，需要指定10位的机器码，这个可以根据自身业务而定即可。例如：机房号+机器号、机器号+业务号、等等可以区别标识的10位比特位的整数值都是可以的。



雪花算法的优点

1. 高并发分布式环境下，生成不重复的id，每秒可生产百万个不重复的id
2. 基于时间戳、以及同一时间戳下序列号自增，基本保证id有序递增
3. 不依赖第三方库或中间件
4. 算法简单，在内存中进行，效率高



雪花算法的缺点

依赖服务器时间，服务器时钟回拨时可能会生成重复id。算法中可通过记录最后一个生成id时的时间戳来解决，每次生成id之前比较当前服务器时钟是否被回拨，避免生成重复id





snowflake算法服务，会判断一下，当前这个请求是否是，机房17的机器25，在2175/11/7 12:12:14时间点发送过来的第一个请求，如果是第一个请求则生成一个id

 

假设，在2175/11/7 12:12:14.000时间里，机房17的机器25，发送了第二条消息，snowflake算法服务，会发现说机房17的机器25，在2175/11/7 12:12:14.000时间里，在这一毫秒，之前已经生成过一个id了，此时如果你同一个机房，同一个机器，在同一个毫秒内，再次要求生成一个id，此时id要加1

![image-20220510220715186](https://pic-typora-qc.oss-cn-chengdu.aliyuncs.com/sharding_img/202205102207321.png)

https://bbs.huaweicloud.com/blogs/344958

https://zhuanlan.zhihu.com/p/402822041

~~~java
@Data
public class IdWorker{

    private long workerId;//机房的机器
    private long datacenterId;//机房
    private long sequence;
    
    
    
    //初始时间戳(纪元） 一般为当前时间2022-05-10 22:59:27，或雪花算法服务上线的时间戳值
    private long initEpoch = 1652194767190L;
    
    // 记录最后使用的毫秒时间戳，主要用于判断是否同一毫秒，以及用于服务器时钟回拨判断
    private long lastTimestamp = -1L;
    
	//datacenterId占用的位数（机房）
    private long datacenterIdBits = 5L;
    //workerId占用的位数（机器）
    private long workerIdBits = 5L;
    
   // dataCenterId占用5个比特位，最大值31
   // 0000000000000000000000000000000000000000000000000000000000011111
    // 这个是二进制运算，就是5 bit最多只能有31个数字，也就是说机器id最多只能是32以内
    private long maxWorkerId = -1L ^ (-1L << workerIdBits); 
   
     // workId占用5个比特位，最大值31
     // 0000000000000000000000000000000000000000000000000000000000011111
     // 这个是一个意思，就是5 bit最多只能有31个数字，机房id最多只能是32以内
    private long maxDatacenterId = -1L ^ (-1L << datacenterIdBits);
    
    //序列号，最后12位
    private long sequenceBits = 12L;
    
    
     // 同一毫秒内的最新序号，最大值可为 2^12 - 1 = 4095
    private long sequenceMask = -1L ^ (-1L << sequenceBits);
    
	//workerId需要左移的位数（12）
    private long workerIdShift = sequenceBits;
    //datacenterId需要左移的位数（12+5）
    private long datacenterIdShift = sequenceBits + workerIdBits;
    //时间戳需要左移的位数：12+5+5
    private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits;
   
    
    
    

    //构造函数seq参数可要可不要
    public IdWorker(long datacenterId, long workerId, long sequence){
		// 检查传递进来的机房id和机器id不能超过32，不能小于0
        if (workerId > maxWorkerId || workerId < 0) {
            throw new IllegalArgumentException(String.format("worker Id can't be greater than %d or less than 0",maxWorkerId));
        }
        if (datacenterId > maxDatacenterId || datacenterId < 0) {
            throw new IllegalArgumentException(String.format("datacenter Id can't be greater than %d or less than 0",maxDatacenterId));
        }
        this.workerId = workerId;
        this.datacenterId = datacenterId;
        this.sequence = sequence;
    }
	

  
    public long getTimestamp(){
        return System.currentTimeMillis();
    }

   /**
    *通过雪花算法生成下一个id,需要进行同步
    *
    *@return 唯一的id
    */
public synchronized long nextId() {
		// 获取当前时间戳，单位是毫秒
        long timestamp = System.currentTimeMillis();;

       // 当前时间小于上一次生成id使用的时间，可能出现服务器时钟回拨问题
        if (timestamp < lastTimestamp) {
            System.err.printf("可能出现服务器时钟回拨问题，请检查服务器时间。当前服务器时间戳：%d，上一次使用时间戳：%d", lastTimestamp);
            throw new RuntimeException(String.format("Clock moved backwards.  Refusing to generate id for %d milliseconds",
                    lastTimestamp - timestamp));
        }


       // 还是在同一毫秒内，，又发送了一个请求生成一个id，则将序列号递增1，序列号最大值为4095
        if (lastTimestamp == timestamp) {
            // 这个意思是说一个毫秒内最多只能有4096个数字，无论你传递多少进来，这个位运算保证始终就是在4096这个范围内，
            //避免你自己传递个sequence超过了4096这个范围
            
            // 序列号的最大值是4095，使用掩码（最低12位为1，高位都为0）进行位与运行后如果值为0，则自增后的序列号超过了4095
      		// 那么就使用新的时间戳
            sequence = (sequence + 1) & sequenceMask; 
            if (sequence == 0) {
                timestamp = tilNextMillis(lastTimestamp);
            }
        } else {
            sequence = 0;// 不在同一毫秒内，则序列号重新从0开始，序列号最大值为4095
        }

	   // 记录一下最近一次生成id的时间戳，单位是毫秒
        lastTimestamp = timestamp;

    
      // 核心算法，将不同部分的数值移动到指定的位置，然后进行或运行
    
       // 这儿就是将时间戳左移，放到41 bit那儿；将机房id左移放到5 bit那儿；将机器id左移放到5 bit那儿；将序号放最后10 bit；最后拼接起来成一个64 bit的二进制数字，转换成10进制就是个long型
        return ((timestamp - initEpoch) << timestampLeftShift) |
                (datacenterId << datacenterIdShift) |
                (workerId << workerIdShift) |
                sequence;
    }

//  0 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000000


    /**
   * 获取指定时间戳的接下来的时间戳，也可以说是下一毫秒
   *
   * @param lastTimeMillis 指定毫秒时间戳
   * @return 时间戳
   */
    private long tilNextMillis(long lastTimestamp) {
        long timestamp =System.currentTimeMillis();
        while (timestamp <= lastTimestamp) {
            timestamp =System.currentTimeMillis();
        }
        return timestamp;
    }

    
    
    
    
    //---------------测试---------------
    public static void main(String[] args) {
        IdWorker worker = new IdWorker(1,1,1);
        for (int i = 0; i < 30; i++) {
            System.out.println(worker.nextId());
        }
        
    // 生成50个id
    Set<Long> set = new TreeSet<>();
    for (int i = 0; i < 50; i++) {
      set.add(worker.nextId());
    }
    System.out.println(set.size());
    System.out.println(set);
    }
    
    // 验证生成100万个id需要多久。大概只花了300毫秒！！！牛逼
    long startTime = System.currentTimeMillis();
    for (int i = 0; i < 1000000; i++) {
      worker.nextId();
    }
    System.out.println(System.currentTimeMillis() - startTime);

}

~~~





大概这个意思就是说41 bit，就是当前毫秒单位的一个时间戳，就这意思；然后5 bit是你传递进来的一个机房id（但是最大只能是32以内），5 bit是你传递进来的机器id（但是最大只能是32以内），剩下的那个10 bit序列号，就是如果跟你上次生成id的时间还在一个毫秒内，那么会把顺序给你累加，最多在4096个序号以内。

 

所以你自己利用这个工具类，自己搞一个服务，然后对每个机房的每个机器都初始化这么一个东西，刚开始这个机房的这个机器的序号就是0。然后每次接收到一个请求，说这个机房的这个机器要生成一个id，你就找到对应的Worker，生成。

 

他这个算法生成的时候，会把当前毫秒放到41 bit中，然后5 bit是机房id，5 bit是机器id，接着就是判断上一次生成id的时间如果跟这次不一样，序号就自动从0开始；要是上次的时间跟现在还是在一个毫秒内，他就把seq累加1，就是自动生成一个毫秒的不同的序号。

 

这个算法那，可以确保说每个机房每个机器每一毫秒，最多生成4096个不重复的id。

 

利用这个snowflake算法，你可以开发自己公司的服务，甚至对于机房id和机器id，反正给你预留了5 bit + 5 bit，你换成别的有业务含义的东西也可以的。

 

这个snowflake算法相对来说还是比较靠谱的，所以你要真是搞分布式id生成，如果是高并发啥的，那么用这个应该性能比较好，一般每秒几万并发的场景，也足够你用了









